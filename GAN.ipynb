{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f383662-bcde-4d47-b238-866c89f13405",
   "metadata": {},
   "source": [
    "# Generating bird sounds with a VAE + GAN loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab492244-39a1-4fca-9863-00f4c174b140",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607baa89-7225-453e-a54a-6618505e07d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T16:39:24.923731Z",
     "iopub.status.busy": "2024-03-12T16:39:24.923430Z",
     "iopub.status.idle": "2024-03-12T16:39:29.050682Z",
     "shell.execute_reply": "2024-03-12T16:39:29.049997Z",
     "shell.execute_reply.started": "2024-03-12T16:39:24.923705Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Audio\n",
    "from PIL import Image\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch import loggers as pl_loggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f71ba-be56-459c-a755-333a60f44bb1",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a221bf20-b88f-415e-9366-3a6e520fcb8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T16:39:29.053172Z",
     "iopub.status.busy": "2024-03-12T16:39:29.052731Z",
     "iopub.status.idle": "2024-03-12T16:39:29.065454Z",
     "shell.execute_reply": "2024-03-12T16:39:29.064604Z",
     "shell.execute_reply.started": "2024-03-12T16:39:29.053151Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        bird_name=None,\n",
    "        transform=None,\n",
    "        num_samples=65_500,\n",
    "        min_db=-80,\n",
    "        max_db=0,\n",
    "        cache=True\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "\n",
    "        if cache:\n",
    "            self.cache_dir = os.path.join(os.path.dirname(os.path.abspath(self.root_dir)), 'cache')\n",
    "            os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        else:\n",
    "            self.cache_dir = None\n",
    "        self.bird_folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "        if bird_name is not None:\n",
    "            self.bird_folders = [bird_name]\n",
    "\n",
    "        self.audio_files = []\n",
    "\n",
    "        for bird_folder in self.bird_folders:\n",
    "            bird_path = os.path.join(root_dir, bird_folder)\n",
    "            audio_files = [os.path.join(bird_path, file) for file in os.listdir(bird_path) if file.endswith('.ogg')]\n",
    "            self.audio_files.extend(audio_files)\n",
    "\n",
    "    def get_spec(self, audio_path):\n",
    "        waveform, sample_rate = librosa.load(audio_path, sr=None, mono=True)\n",
    "\n",
    "        if len(waveform) < self.num_samples:\n",
    "            pad_amount = self.num_samples - len(waveform)\n",
    "            waveform = np.pad(waveform, (0, pad_amount))\n",
    "        else:\n",
    "            waveform = waveform[:self.num_samples]\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=waveform, sr=sample_rate)\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        return mel_spec\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x- self.min_db) / (self.max_db - self.min_db)\n",
    "\n",
    "    def denormalize(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.cpu().detach().numpy()\n",
    "\n",
    "        flattened_array = x.reshape((x.shape[0], -1))\n",
    "\n",
    "        min_batch_values = flattened_array.min(axis=-1, keepdims=True)\n",
    "        max_batch_values = flattened_array.max(axis=-1, keepdims=True)\n",
    "\n",
    "        normalized_array = self.min_db + ((flattened_array - min_batch_values) / (max_batch_values - min_batch_values)) * (self.max_db - self.min_db)\n",
    "\n",
    "        normalized_batch = normalized_array.reshape(x.shape)\n",
    "\n",
    "        return normalized_batch\n",
    "\n",
    "    def cache_all(self):\n",
    "        self.cache = True\n",
    "        for idx in range(len(self)):\n",
    "            self.__getitem__(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "\n",
    "        if self.cache_dir is not None:\n",
    "            cache_filename = f\"{os.path.basename(audio_path)}_{self.num_samples}.npy\"\n",
    "            cache_path = os.path.join(self.cache_dir, cache_filename)\n",
    "\n",
    "        if self.cache_dir is not None and os.path.isfile(cache_path):\n",
    "            try:\n",
    "                mel_spec = np.load(cache_path)\n",
    "            except Exception as e:\n",
    "                raise IOError(f\"Failed to read file: {cache_path}. Error: {e}\")\n",
    "        else:\n",
    "            mel_spec = self.get_spec(audio_path)\n",
    "\n",
    "            # Normalize mel_spec\n",
    "            mel_spec = self.normalize(mel_spec)\n",
    "            mel_spec = np.expand_dims(mel_spec, axis=0)\n",
    "\n",
    "            # Save mel spectrogram to cache\n",
    "            if self.cache_dir is not None:\n",
    "                np.save(cache_path, mel_spec)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            mel_spec = self.transform(mel_spec)\n",
    "\n",
    "        folder, filename = os.path.split(audio_path)\n",
    "        basedir, bird = os.path.split(folder)\n",
    "\n",
    "        return mel_spec, bird, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e7f6888-92a3-47f4-a719-ec97e87c7ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T16:39:29.066571Z",
     "iopub.status.busy": "2024-03-12T16:39:29.066377Z",
     "iopub.status.idle": "2024-03-12T16:39:29.089602Z",
     "shell.execute_reply": "2024-03-12T16:39:29.088919Z",
     "shell.execute_reply.started": "2024-03-12T16:39:29.066552Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 batch_size=64,\n",
    "                 validation_split=0.2,\n",
    "                 num_workers=10,\n",
    "                 bird_name=None,\n",
    "                 transform=None,\n",
    "                 num_samples=65_500,\n",
    "                 min_db=-80,\n",
    "                 max_db=0,\n",
    "                 cache=True,\n",
    "                 seed=0\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.validation_split = validation_split\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "        self.cache = cache\n",
    "        self.seed = seed\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = BirdClefDataset(self.root_dir, bird_name=self.bird_name)\n",
    "        self.normalize = dataset.normalize\n",
    "        self.denormalize = dataset.denormalize\n",
    "        if stage == 'fit' or stage is None:\n",
    "            train_dataset, validation_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                              (1 - self.validation_split, self.validation_split),\n",
    "                                                                             torch.Generator().manual_seed(self.seed))\n",
    "            self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "            self.validation_loader = DataLoader(validation_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.validation_loader\n",
    "\n",
    "    def illustration_dataloader(self, batch_size):\n",
    "        illustrative_dataset = self.validation_loader.dataset\n",
    "        illustrative_loader = DataLoader(illustrative_dataset, batch_size=batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        return illustrative_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a05391-c206-442b-b32b-a2f42de612da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T16:39:29.090972Z",
     "iopub.status.busy": "2024-03-12T16:39:29.090524Z",
     "iopub.status.idle": "2024-03-12T16:39:29.098465Z",
     "shell.execute_reply": "2024-03-12T16:39:29.097771Z",
     "shell.execute_reply.started": "2024-03-12T16:39:29.090942Z"
    }
   },
   "outputs": [],
   "source": [
    "root_directory = 'train_audio'\n",
    "data_module = BirdClefDataModule(root_directory, batch_size=128)#, bird_name=\"rerswa1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb1e1e-5fa2-4d02-8bdd-99383247594e",
   "metadata": {},
   "source": [
    "## VAE-GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d169659-0b0b-4ac1-9063-b1428937d99f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T16:39:29.099920Z",
     "iopub.status.busy": "2024-03-12T16:39:29.099651Z",
     "iopub.status.idle": "2024-03-12T16:39:29.105840Z",
     "shell.execute_reply": "2024-03-12T16:39:29.104903Z",
     "shell.execute_reply.started": "2024-03-12T16:39:29.099891Z"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def __init__(self, size=16, channels=128):\n",
    "        super(UnFlatten, self).__init__()\n",
    "        self.size = size\n",
    "        self.channels = channels\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), self.channels, self.size, self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab161ae6-b928-453f-8b06-b8c5c698d2f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T16:39:29.107165Z",
     "iopub.status.busy": "2024-03-12T16:39:29.106893Z",
     "iopub.status.idle": "2024-03-12T16:39:29.114615Z",
     "shell.execute_reply": "2024-03-12T16:39:29.113828Z",
     "shell.execute_reply.started": "2024-03-12T16:39:29.107135Z"
    }
   },
   "outputs": [],
   "source": [
    "class GANLoss(torch.nn.modules.loss._Loss):\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean'):\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "\n",
    "    def forward(self, sampled_classif, original_classif):\n",
    "        loss_discriminator_samples = F.binary_cross_entropy_with_logits(\n",
    "            sampled_classif,\n",
    "            torch.ones_like(sampled_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "        loss_discriminator_original = F.binary_cross_entropy_with_logits(\n",
    "            original_classif,\n",
    "            torch.zeros_like(original_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "        loss_discriminator = 1/2 * (loss_discriminator_samples + loss_discriminator_original)\n",
    "\n",
    "        loss_generator = F.binary_cross_entropy_with_logits(\n",
    "            sampled_classif,\n",
    "            torch.zeros_like(sampled_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            loss_discriminator,\n",
    "            loss_generator,\n",
    "            loss_discriminator_samples,\n",
    "            loss_discriminator_original\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cbe3d27-24c8-47ce-91ed-727167823d67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T16:39:29.116318Z",
     "iopub.status.busy": "2024-03-12T16:39:29.116108Z",
     "iopub.status.idle": "2024-03-12T16:39:29.152483Z",
     "shell.execute_reply": "2024-03-12T16:39:29.151690Z",
     "shell.execute_reply.started": "2024-03-12T16:39:29.116297Z"
    }
   },
   "outputs": [],
   "source": [
    "class GAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_channels=1,\n",
    "        img_size=256,\n",
    "        input_size=4096,\n",
    "        layers=[16, 32, 64],\n",
    "        learning_rate=0.001,\n",
    "        lr_decay=1,\n",
    "        activation=nn.ReLU,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        generate_on_epoch=4,\n",
    "        generator_too_good=.6, # value of the discriminator's loss above which the generator shouldn't be trained\n",
    "        discriminator_too_good=.3, # value of the discriminator's loss above which the discriminator shouldn't be trained\n",
    "        seed=0\n",
    "    ):\n",
    "        super(GAN, self).__init__()\n",
    "\n",
    "        if img_size % (2**len(layers)) != 0:\n",
    "            raise ValueError(\"An image of size {image_size} with {len(layers)} layers won't work\")\n",
    "\n",
    "        self.input_size = input_size\n",
    "        last_conv_size = (img_size // (2**(len(layers)+1)))**2 * layers[-1]\n",
    "        self.generator = self.build_generator(img_channels, layers, img_size, last_conv_size, activation)\n",
    "        self.discriminator = self.build_discriminator(img_channels, layers, activation, last_conv_size)\n",
    "\n",
    "        self.generator_too_good = generator_too_good\n",
    "        self.discriminator_too_good = discriminator_too_good\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.optim = optimizer\n",
    "        self.criterion = GANLoss()\n",
    "        self.generate_on_epoch = generate_on_epoch\n",
    "        self.seed = seed\n",
    "\n",
    "        self.prev_disc_loss = None\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def build_discriminator(self, input_channels, channels_list, activation, last_conv_size):\n",
    "        layers = []\n",
    "        in_channels = input_channels\n",
    "        for out_channels in channels_list:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(activation())\n",
    "            in_channels = out_channels\n",
    "        layers.append(Flatten())\n",
    "        layers.append(nn.Linear(last_conv_size, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def build_generator(self, input_channels, channels_list, img_size, last_conv_size, activation):\n",
    "        layers = [\n",
    "            nn.Linear(self.input_size, last_conv_size),\n",
    "            UnFlatten(img_size // (2**(len(channels_list)+1)), channels_list[-1])\n",
    "        ]\n",
    "        for in_channels, out_channels in zip(channels_list[::-1], channels_list[-2::-1]+[input_channels]):\n",
    "            layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "            layers.append(activation())\n",
    "            #layers.append(nn.Upsample(scale_factor=2, mode=\"bilinear\"))\n",
    "            #layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "            #layers.append(activation())\n",
    "        layers.pop()\n",
    "        layers.append(nn.Sigmoid())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def sample(self, batch_size, seed=None):\n",
    "        if seed is None:\n",
    "            z = torch.randn((batch_size, self.input_size), device=self.device)\n",
    "        else:\n",
    "            gen = torch.Generator(device=self.device).manual_seed(seed)\n",
    "            z = torch.empty((batch_size, self.input_size), device=self.device).normal_(generator=gen)\n",
    "        return z\n",
    "\n",
    "    def generate(self, x):\n",
    "        return self.generator(x)\n",
    "\n",
    "    def forward(self, originals):\n",
    "        # generate\n",
    "        z = self.sample(originals.shape[0])\n",
    "        generated = self.generate(z)\n",
    "\n",
    "        # discriminate\n",
    "        logits_originals = self.discriminator(originals)\n",
    "        logits_generated = self.discriminator(generated)\n",
    "\n",
    "        return generated, logits_originals, logits_generated\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if isinstance(self.learning_rate, tuple):\n",
    "            lr_generator, lr_discriminator = self.learning_rate\n",
    "        else:\n",
    "            lr_generator = lr_discriminator = self.learning_rate\n",
    "\n",
    "        if isinstance(self.lr_decay, tuple):\n",
    "            lr_decay_generator, lr_decay_discriminator = self.lr_decay\n",
    "        else:\n",
    "            lr_decay_generator = lr_decay_discriminator = self.lr_decay\n",
    "\n",
    "        optimizer_generator = self.optim(self.generator.parameters(), lr=lr_generator)\n",
    "        optimizer_discriminator = self.optim(self.discriminator.parameters(), lr=lr_discriminator)\n",
    "\n",
    "        scheduler_generator = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer_generator, gamma=lr_decay_generator),\n",
    "        }\n",
    "        scheduler_discriminator = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer_discriminator, gamma=lr_decay_discriminator),\n",
    "        }\n",
    "\n",
    "        return (\n",
    "            [\n",
    "                optimizer_generator,\n",
    "                optimizer_discriminator\n",
    "            ],\n",
    "            [\n",
    "                scheduler_discriminator,\n",
    "                scheduler_discriminator\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        optimizer_generator, optimizer_discriminator = self.optimizers()\n",
    "        #self.toggle_optimizer(optimizer_encoder)\n",
    "        #self.toggle_optimizer(optimizer_decoder)\n",
    "        #self.toggle_optimizer(optimizer_discriminator)\n",
    "        x, birdname, file = batch\n",
    "        generated, logits_originals, logits_generated = self(x)\n",
    "        loss_discriminator, loss_generator, loss_discriminator_samples, loss_discriminator_original = self.criterion(logits_generated, logits_originals)\n",
    "\n",
    "        optimizer_generator.zero_grad()\n",
    "        optimizer_discriminator.zero_grad()\n",
    "\n",
    "        # we apply the loss to the generator and then prevent it from being affected by other losses\n",
    "        loss_generator.backward(retain_graph=True)\n",
    "        for group in optimizer_generator.param_groups:\n",
    "            for param in group['params']:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # we reset the discriminator that has been affected by the generator's loss\n",
    "        optimizer_discriminator.zero_grad()\n",
    "\n",
    "        loss_discriminator.backward()\n",
    "\n",
    "        if self.prev_disc_loss is None or self.prev_disc_loss < self.generator_too_good:\n",
    "            optimizer_generator.step()\n",
    "            self.log('train/active/generator', 1, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "        else:\n",
    "            self.log('train/active/generator', 0, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "        if self.prev_disc_loss is None or self.prev_disc_loss > self.discriminator_too_good:\n",
    "            optimizer_discriminator.step()\n",
    "            self.log('train/active/discriminator', 1, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "        else:\n",
    "            self.log('train/active/discriminator', 0, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        self.prev_disc_loss = loss_discriminator.detach()\n",
    "\n",
    "        # resettings things to thier normal states\n",
    "        for group in optimizer_generator.param_groups:\n",
    "            for param in group['params']:\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.log('train/loss_generator', loss_generator, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_discriminator', loss_discriminator, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        self.log('train/loss_discriminator_samples', loss_discriminator_samples, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_discriminator_original', loss_discriminator_original, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        tensorboard = self.logger.experiment\n",
    "\n",
    "        tensorboard.add_histogram(\n",
    "            \"train/pred_originals\",\n",
    "            F.sigmoid(logits_originals),\n",
    "            self.trainer.num_training_batches * self.current_epoch + batch_idx\n",
    "        )\n",
    "\n",
    "        tensorboard.add_histogram(\n",
    "            \"trian/pred_generated\",\n",
    "            F.sigmoid(logits_generated),\n",
    "            self.trainer.num_training_batches* self.current_epoch + batch_idx\n",
    "        )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, birdname, file = batch\n",
    "        generated, logits_originals, logits_generated = self(x)\n",
    "        loss_discriminator, loss_generator, loss_discriminator_samples, loss_discriminator_original = self.criterion(logits_generated, logits_originals)\n",
    "\n",
    "        self.log('validation/loss_generator', loss_generator, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_discriminator', loss_discriminator, on_epoch=True, on_step=False, batch_size=x.shape[0])\n",
    "\n",
    "        self.log('validation/loss_discriminator_samples', loss_discriminator_samples, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_discriminator_original', loss_discriminator_original, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "\n",
    "        tensorboard = self.logger.experiment\n",
    "\n",
    "        tensorboard.add_histogram(\n",
    "            \"validation/pred_originals\",\n",
    "            F.sigmoid(logits_originals),\n",
    "            self.trainer.num_val_batches[0] * self.current_epoch + batch_idx\n",
    "        )\n",
    "        tensorboard.add_histogram(\n",
    "            \"validation/pred_generated\",\n",
    "            F.sigmoid(logits_generated),\n",
    "            self.trainer.num_val_batches[0] * self.current_epoch + batch_idx\n",
    "        )\n",
    "\n",
    "    def generate_specs(self, n=None):\n",
    "        if n is None:\n",
    "            n = self.trainer.datamodule.batch_size\n",
    "        z = self.sample(n, seed=0)\n",
    "        specs = self.generate(z)\n",
    "\n",
    "        return self.trainer.datamodule.denormalize(specs[:, 0])\n",
    "\n",
    "    def spec_to_img(self, spec):\n",
    "        fig, ax = plt.subplots()\n",
    "        img = librosa.display.specshow(spec, x_axis='time', y_axis='mel', sr=32000, ax=ax)\n",
    "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "        ax.set(title='Mel-frequency spectrogram')\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png')\n",
    "        buffer.seek(0)\n",
    "\n",
    "        plt.close(fig)\n",
    "        image = Image.open(buffer)\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1)  # Convert to tensor and adjust dimensions\n",
    "\n",
    "        return image_tensor\n",
    "\n",
    "    def on_validation_end(self):\n",
    "        tensorboard = self.logger.experiment\n",
    "\n",
    "        specs = self.generate_specs(self.generate_on_epoch)\n",
    "        for i, spec in enumerate(specs):\n",
    "            tensorboard.add_image(f\"generated_spectrogram_{i}\", self.spec_to_img(spec), self.global_step)\n",
    "            audio = librosa.feature.inverse.mel_to_audio(spec)\n",
    "            tensorboard.add_audio(f\"generated_audio_{i}\", audio, self.global_step, 32000)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        for scheduler in self.lr_schedulers():\n",
    "            scheduler.step()\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.logger.log_hyperparams(self.hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3d60e7-0d75-4fdd-96a5-ac2fb2194352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T16:39:29.153358Z",
     "iopub.status.busy": "2024-03-12T16:39:29.153176Z",
     "iopub.status.idle": "2024-03-12T16:39:29.467688Z",
     "shell.execute_reply": "2024-03-12T16:39:29.466911Z",
     "shell.execute_reply.started": "2024-03-12T16:39:29.153340Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "num_epochs = 100\n",
    "\n",
    "pl.seed_everything(seed, workers=True)\n",
    "\n",
    "model = GAN(\n",
    "    img_channels=1,\n",
    "    img_size=256,\n",
    "    input_size=8192,\n",
    "    layers=[32, 64, 128, 256, 256],\n",
    "    learning_rate=1e-4,\n",
    "    lr_decay=(1/100)**(1/num_epochs),\n",
    "    seed=seed,\n",
    "    generator_too_good=.65,\n",
    "    discriminator_too_good=.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e185172-2f0d-41c1-9d7d-a75d9615e518",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7cdfa7-4b05-42ae-9dcd-6337b6bf751c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T16:39:29.468973Z",
     "iopub.status.busy": "2024-03-12T16:39:29.468682Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | generator     | Sequential | 34.5 M\n",
      "1 | discriminator | Sequential | 982 K \n",
      "2 | criterion     | GANLoss    | 0     \n",
      "---------------------------------------------\n",
      "35.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "35.5 M    Total params\n",
      "142.073   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateo/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv_transpose2d(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab638996bde94186a2aeeac8564f5156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: audio amplitude out of range, auto clipped.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"gan/\", default_hp_metric=False)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator='auto',\n",
    "    log_every_n_steps=1,\n",
    "    logger=tb_logger,\n",
    "    deterministic=True,\n",
    "    callbacks=[lr_monitor]\n",
    ")\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b59ac5-1378-4fb5-b7a8-d91a921bcb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e0572-8b6e-4b59-a557-81ec536d9e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
