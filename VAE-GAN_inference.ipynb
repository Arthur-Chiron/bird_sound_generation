{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9ff7ca-7410-4e5d-aec3-ee0c470ae250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T23:03:04.409849Z",
     "iopub.status.busy": "2024-03-14T23:03:04.409564Z",
     "iopub.status.idle": "2024-03-14T23:03:10.395938Z",
     "shell.execute_reply": "2024-03-14T23:03:10.395143Z",
     "shell.execute_reply.started": "2024-03-14T23:03:04.409825Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Audio\n",
    "from PIL import Image\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch import loggers as pl_loggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ac407-8e06-438f-a051-37cfa6ce8c36",
   "metadata": {},
   "source": [
    "## Dataset embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29643a79-d117-48c6-8cde-bc9aef1bda77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T23:03:10.397451Z",
     "iopub.status.busy": "2024-03-14T23:03:10.397124Z",
     "iopub.status.idle": "2024-03-14T23:03:10.413128Z",
     "shell.execute_reply": "2024-03-14T23:03:10.412538Z",
     "shell.execute_reply.started": "2024-03-14T23:03:10.397433Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        bird_name=None,\n",
    "        transform=None,\n",
    "        num_samples=65_500,\n",
    "        min_db=-80,\n",
    "        max_db=0,\n",
    "        cache=True\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "\n",
    "        if cache:\n",
    "            self.cache_dir = self.root_dir + \"_cache\"\n",
    "            os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        else:\n",
    "            self.cache_dir = None\n",
    "        self.bird_folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "        if bird_name is not None:\n",
    "            self.bird_folders = [bird_name]\n",
    "\n",
    "        self.audio_files = []\n",
    "\n",
    "        for bird_folder in self.bird_folders:\n",
    "            bird_path = os.path.join(root_dir, bird_folder)\n",
    "            audio_files = [os.path.join(bird_path, file) for file in os.listdir(bird_path) if file.endswith('.ogg')]\n",
    "            self.audio_files.extend(audio_files)\n",
    "\n",
    "    def get_spec(self, audio_path):\n",
    "        waveform, sample_rate = librosa.load(audio_path, sr=None, mono=True)\n",
    "\n",
    "        if len(waveform) < self.num_samples:\n",
    "            pad_amount = self.num_samples - len(waveform)\n",
    "            waveform = np.pad(waveform, (0, pad_amount))\n",
    "        else:\n",
    "            waveform = waveform[:self.num_samples]\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=waveform, sr=sample_rate)\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        return mel_spec\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x- self.min_db) / (self.max_db - self.min_db)\n",
    "\n",
    "    def denormalize(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.cpu().detach().numpy()\n",
    "\n",
    "        flattened_array = x.reshape((x.shape[0], -1))\n",
    "\n",
    "        min_batch_values = flattened_array.min(axis=-1, keepdims=True)\n",
    "        max_batch_values = flattened_array.max(axis=-1, keepdims=True)\n",
    "\n",
    "        normalized_array = self.min_db + ((flattened_array - min_batch_values) / (max_batch_values - min_batch_values)) * (self.max_db - self.min_db)\n",
    "\n",
    "        normalized_batch = normalized_array.reshape(x.shape)\n",
    "\n",
    "        return normalized_batch\n",
    "\n",
    "    def cache_all(self):\n",
    "        self.cache = True\n",
    "        for idx in range(len(self)):\n",
    "            self.__getitem__(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "\n",
    "        if self.cache_dir is not None:\n",
    "            cache_filename = f\"{os.path.basename(audio_path)}_{self.num_samples}.npy\"\n",
    "            cache_path = os.path.join(self.cache_dir, cache_filename)\n",
    "\n",
    "        if self.cache_dir is not None and os.path.isfile(cache_path):\n",
    "            try:\n",
    "                mel_spec = np.load(cache_path)\n",
    "            except Exception as e:\n",
    "                raise IOError(f\"Failed to read file: {cache_path}. Error: {e}\")\n",
    "        else:\n",
    "            mel_spec = self.get_spec(audio_path)\n",
    "\n",
    "            # Normalize mel_spec\n",
    "            mel_spec = self.normalize(mel_spec)\n",
    "            mel_spec = np.expand_dims(mel_spec, axis=0)\n",
    "\n",
    "            # Save mel spectrogram to cache\n",
    "            if self.cache_dir is not None:\n",
    "                np.save(cache_path, mel_spec)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            mel_spec = self.transform(mel_spec)\n",
    "\n",
    "        folder, filename = os.path.split(audio_path)\n",
    "        basedir, bird = os.path.split(folder)\n",
    "\n",
    "        return mel_spec, bird, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e8fa23-2691-445a-8ca2-8caba122de27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T23:03:10.414549Z",
     "iopub.status.busy": "2024-03-14T23:03:10.414058Z",
     "iopub.status.idle": "2024-03-14T23:03:10.425751Z",
     "shell.execute_reply": "2024-03-14T23:03:10.425094Z",
     "shell.execute_reply.started": "2024-03-14T23:03:10.414519Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 batch_size=64,\n",
    "                 validation_split=0.2,\n",
    "                 num_workers=10,\n",
    "                 bird_name=None,\n",
    "                 transform=None,\n",
    "                 num_samples=65_500,\n",
    "                 min_db=-80,\n",
    "                 max_db=0,\n",
    "                 cache=True,\n",
    "                 seed=0\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.validation_split = validation_split\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "        self.cache = cache\n",
    "        self.seed = seed\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = BirdClefDataset(self.root_dir, bird_name=self.bird_name)\n",
    "        self.normalize = dataset.normalize\n",
    "        self.denormalize = dataset.denormalize\n",
    "        if stage == 'fit' or stage is None:\n",
    "            train_dataset, validation_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                              (1 - self.validation_split, self.validation_split),\n",
    "                                                                             torch.Generator().manual_seed(self.seed))\n",
    "            self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "            self.validation_loader = DataLoader(validation_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.validation_loader\n",
    "\n",
    "    def illustration_dataloader(self, batch_size):\n",
    "        illustrative_dataset = self.validation_loader.dataset\n",
    "        illustrative_loader = DataLoader(illustrative_dataset, batch_size=batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        return illustrative_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659f12ae-13d7-46e5-802e-07e0bbb0d3f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T23:03:10.427090Z",
     "iopub.status.busy": "2024-03-14T23:03:10.426586Z",
     "iopub.status.idle": "2024-03-14T23:03:10.474957Z",
     "shell.execute_reply": "2024-03-14T23:03:10.474252Z",
     "shell.execute_reply.started": "2024-03-14T23:03:10.427060Z"
    }
   },
   "outputs": [],
   "source": [
    "root_directory = 'train_audio'\n",
    "data_module = BirdClefDataModule(root_directory, batch_size=128)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcba302-7e86-43cd-88b8-498171c9faaf",
   "metadata": {},
   "source": [
    "## VAE inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e4f3dc-3199-4436-b448-103d2f69bf18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T23:03:10.476027Z",
     "iopub.status.busy": "2024-03-14T23:03:10.475781Z",
     "iopub.status.idle": "2024-03-14T23:03:10.481039Z",
     "shell.execute_reply": "2024-03-14T23:03:10.480178Z",
     "shell.execute_reply.started": "2024-03-14T23:03:10.476006Z"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def __init__(self, size=16, channels=128):\n",
    "        super(UnFlatten, self).__init__()\n",
    "        self.size = size\n",
    "        self.channels = channels\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), self.channels, self.size, self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba40048-eb2d-4d37-9c93-a2519f8f90fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T23:03:10.482412Z",
     "iopub.status.busy": "2024-03-14T23:03:10.482079Z",
     "iopub.status.idle": "2024-03-14T23:03:10.492210Z",
     "shell.execute_reply": "2024-03-14T23:03:10.491619Z",
     "shell.execute_reply.started": "2024-03-14T23:03:10.482380Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAEGANLoss(torch.nn.modules.loss._Loss):\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, beta=1.0, size_average=None, reduce=None, reduction: str = 'mean', reconstruction_loss=F.mse_loss, reconstruction_decoder_weight=100):\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "        self.beta = beta\n",
    "        self.reconstruction_loss = reconstruction_loss\n",
    "        self.reconstruction_decoder_weight = reconstruction_decoder_weight\n",
    "\n",
    "    def forward(self, reconstructed_x, x, mean, logvar, sampled_classif, reconstructed_classif, original_classif):\n",
    "        reconstruction_loss = self.reconstruction_loss(reconstructed_x, x, reduction=self.reduction)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp(), dim=1)\n",
    "        if self.reduction != 'none':\n",
    "            kl_loss = torch.mean(kl_loss) if self.reduction == 'mean' else torch.sum(kl_loss)\n",
    "\n",
    "        loss_encoder = reconstruction_loss + self.beta * kl_loss\n",
    "\n",
    "        loss_discriminator_samples = F.binary_cross_entropy_with_logits(\n",
    "            sampled_classif,\n",
    "            torch.ones_like(sampled_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "        loss_discriminator_original = F.binary_cross_entropy_with_logits(\n",
    "            original_classif,\n",
    "            torch.zeros_like(original_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "        loss_discriminator_reconstructed = F.binary_cross_entropy_with_logits(\n",
    "            reconstructed_classif,\n",
    "            torch.ones_like(reconstructed_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "        loss_discriminator = 1/4 * (loss_discriminator_samples + loss_discriminator_original * 2 + loss_discriminator_reconstructed)\n",
    "\n",
    "\n",
    "        loss_decoder_reconstructed = F.binary_cross_entropy_with_logits(\n",
    "            reconstructed_classif,\n",
    "            torch.zeros_like(reconstructed_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "\n",
    "        loss_decoder_samples = F.binary_cross_entropy_with_logits(\n",
    "            sampled_classif,\n",
    "            torch.zeros_like(sampled_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "\n",
    "        loss_decoder = (loss_decoder_samples +\n",
    "                        loss_decoder_reconstructed +\n",
    "                        reconstruction_loss * self.reconstruction_decoder_weight)/(self.reconstruction_decoder_weight+2)\n",
    "\n",
    "        return (\n",
    "            loss_encoder,\n",
    "            loss_decoder,\n",
    "            loss_discriminator,\n",
    "            reconstruction_loss,\n",
    "            kl_loss,\n",
    "            loss_decoder_reconstructed,\n",
    "            loss_decoder_samples,\n",
    "            loss_discriminator_samples,\n",
    "            loss_discriminator_original,\n",
    "            loss_discriminator_reconstructed\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2019ebe6-7141-46cf-a349-08b63f6ef869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T23:03:10.494248Z",
     "iopub.status.busy": "2024-03-14T23:03:10.493835Z",
     "iopub.status.idle": "2024-03-14T23:03:10.542943Z",
     "shell.execute_reply": "2024-03-14T23:03:10.542171Z",
     "shell.execute_reply.started": "2024-03-14T23:03:10.494217Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAEGAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=1,\n",
    "        img_size=256,\n",
    "        hidden_size=4096,\n",
    "        layers=[16, 32, 64],\n",
    "        learning_rate=0.001,\n",
    "        lr_decay=1,\n",
    "        beta=1e-3,\n",
    "        activation=nn.ReLU,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        reconstruction_loss=F.mse_loss,\n",
    "        reconstruction_decoder_weight=100,\n",
    "        generate_on_epoch=4,\n",
    "        reconstruct_on_epoch=4,\n",
    "        generator_too_good=.6, # value of the discriminator's loss above which the generator shouldn't be trained\n",
    "        discriminator_too_good=.3, # value of the discriminator's loss above which the discriminator shouldn't be trained\n",
    "        seed=0\n",
    "    ):\n",
    "        super(VAEGAN, self).__init__()\n",
    "\n",
    "        if img_size % (2**len(layers)) != 0:\n",
    "            raise ValueError(\"An image of size {image_size} with {len(layers)} layers won't be reconstructed with the correct size\")\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = self.build_encoder(input_channels, layers, activation)\n",
    "        last_conv_size = (img_size // (2**(len(layers)+1)))**2 * layers[-1]\n",
    "        self.mean_layer = nn.Linear(last_conv_size, self.hidden_size)\n",
    "        self.logvar_layer = nn.Linear(last_conv_size, self.hidden_size)\n",
    "        self.decoder = self.build_decoder(input_channels, layers, img_size, last_conv_size, activation)\n",
    "        self.discriminator = self.build_discriminator(input_channels, layers, activation, last_conv_size)\n",
    "\n",
    "        self.generator_too_good = generator_too_good\n",
    "        self.discriminator_too_good = discriminator_too_good\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.optim = optimizer\n",
    "        self.criterion = VAEGANLoss(\n",
    "            beta=beta,\n",
    "            reconstruction_loss=reconstruction_loss,\n",
    "            reconstruction_decoder_weight=reconstruction_decoder_weight\n",
    "        )\n",
    "        self.generate_on_epoch = generate_on_epoch\n",
    "        self.reconstruct_on_epoch = reconstruct_on_epoch\n",
    "        self.seed = seed\n",
    "\n",
    "        self.prev_disc_loss = None\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def build_discriminator(self, input_channels, channels_list, activation, last_conv_size):\n",
    "        layers = []\n",
    "        in_channels = input_channels\n",
    "        for out_channels in channels_list:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(activation())\n",
    "            in_channels = out_channels\n",
    "        layers.append(Flatten())\n",
    "        layers.append(nn.Linear(last_conv_size, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def build_encoder(self, input_channels, channels_list, activation):\n",
    "        layers = []\n",
    "        in_channels = input_channels\n",
    "        for out_channels in channels_list:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(activation())\n",
    "            #layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "            #layers.append(activation())\n",
    "            #layers.append(nn.MaxPool2d(2))\n",
    "            in_channels = out_channels\n",
    "        layers.append(Flatten())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def build_decoder(self, input_channels, channels_list, img_size, last_conv_size, activation):\n",
    "        layers = [\n",
    "            nn.Linear(self.hidden_size, last_conv_size),\n",
    "            UnFlatten(img_size // (2**(len(channels_list)+1)), channels_list[-1])\n",
    "        ]\n",
    "        for in_channels, out_channels in zip(channels_list[::-1], channels_list[-2::-1]+[input_channels]):\n",
    "            layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "            layers.append(activation())\n",
    "            #layers.append(nn.Upsample(scale_factor=2, mode=\"bilinear\"))\n",
    "            #layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "            #layers.append(activation())\n",
    "        layers.pop()\n",
    "        layers.append(nn.Sigmoid())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def sample(self, mean, logvar, seed=None):\n",
    "        std = logvar.exp()\n",
    "        if seed is None:\n",
    "            epsilon = torch.randn_like(std)\n",
    "        else:\n",
    "            gen = torch.Generator(device=self.device).manual_seed(seed)\n",
    "            epsilon = torch.empty_like(std).normal_(generator=gen)\n",
    "        z = mean + std*epsilon\n",
    "        return z\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, originals):\n",
    "        # reconstruct from inputs\n",
    "        mean, logvar = self.encode(originals)\n",
    "        z = self.sample(mean, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "\n",
    "        # generate from scratch\n",
    "        mean_ = torch.zeros([originals.shape[0], self.hidden_size]).to(self.device)\n",
    "        logvar_ = torch.zeros([originals.shape[0], self.hidden_size]).to(self.device)\n",
    "        z_ = self.sample(mean_, logvar_)\n",
    "        sampled = self.decode(z_)\n",
    "\n",
    "        # discriminate\n",
    "        logits_originals = self.discriminator(originals)\n",
    "        logits_reconstructed = self.discriminator(reconstructed)\n",
    "        logits_sampled = self.discriminator(sampled)\n",
    "\n",
    "        return reconstructed, sampled, mean, logvar, logits_originals, logits_reconstructed, logits_sampled\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if isinstance(self.learning_rate, tuple):\n",
    "            lr_encoder, lr_decoder, lr_discriminator = self.learning_rate\n",
    "        else:\n",
    "            lr_encoder = lr_decoder = lr_discriminator = self.learning_rate\n",
    "\n",
    "        if isinstance(self.lr_decay, tuple):\n",
    "            lr_decay_encoder, lr_decay_decoder, lr_decay_discriminator = self.lr_decay\n",
    "        else:\n",
    "            lr_decay_encoder = lr_decay_decoder = lr_decay_discriminator = self.lr_decay\n",
    "\n",
    "        optimizer_encoder = self.optim(\n",
    "            list(self.encoder.parameters()) + list(self.mean_layer.parameters()) + list(self.logvar_layer.parameters()),\n",
    "            lr=lr_encoder\n",
    "        )\n",
    "        optimizer_decoder = self.optim(self.decoder.parameters(), lr=lr_decoder)\n",
    "        optimizer_discriminator = self.optim(self.discriminator.parameters(), lr=lr_discriminator)\n",
    "\n",
    "        scheduler_encoder = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer_encoder, gamma=lr_decay_encoder),\n",
    "        }\n",
    "\n",
    "        scheduler_decoder = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer_decoder, gamma=lr_decay_decoder),\n",
    "        }\n",
    "\n",
    "        scheduler_discriminator = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer_discriminator, gamma=lr_decay_discriminator),\n",
    "        }\n",
    "\n",
    "        return (\n",
    "            [\n",
    "                optimizer_encoder,\n",
    "                optimizer_decoder,\n",
    "                optimizer_discriminator\n",
    "            ],\n",
    "            [\n",
    "                scheduler_encoder,\n",
    "                scheduler_decoder,\n",
    "                scheduler_discriminator\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        optimizer_encoder, optimizer_decoder, optimizer_discriminator = self.optimizers()\n",
    "        #self.toggle_optimizer(optimizer_encoder)\n",
    "        #self.toggle_optimizer(optimizer_decoder)\n",
    "        #self.toggle_optimizer(optimizer_discriminator)\n",
    "        x, birdname, file = batch\n",
    "        reconstructed, sampled, mean, logvar, logits_originals, logits_reconstructed, logits_sampled = self(x)\n",
    "        (loss_encoder,\n",
    "         loss_decoder,\n",
    "         loss_discriminator,\n",
    "         reconstruction_loss,\n",
    "         kl_loss,\n",
    "         loss_decoder_reconstructed,\n",
    "         loss_decoder_samples,\n",
    "         loss_discriminator_samples,\n",
    "         loss_discriminator_original,\n",
    "         loss_discriminator_reconstructed\n",
    "        ) = self.criterion(\n",
    "            reconstructed,\n",
    "            x,\n",
    "            mean,\n",
    "            logvar,\n",
    "                logits_sampled,\n",
    "            logits_reconstructed,\n",
    "            logits_originals\n",
    "        )\n",
    "\n",
    "\n",
    "        optimizer_encoder.zero_grad()\n",
    "        optimizer_decoder.zero_grad()\n",
    "        optimizer_discriminator.zero_grad()\n",
    "\n",
    "        # we apply the loss to the encoder and then prevent it from being affected by other losses\n",
    "        loss_encoder.backward(retain_graph=True)\n",
    "        for group in optimizer_encoder.param_groups:\n",
    "            for param in group['params']:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # we reset the decoder and discriminator that have been affected by the encoder's loss\n",
    "        optimizer_decoder.zero_grad()\n",
    "        optimizer_discriminator.zero_grad()\n",
    "\n",
    "        # we apply the loss to the decoder and then prevent it from being affected by other losses\n",
    "        loss_decoder.backward(retain_graph=True)\n",
    "        for group in optimizer_decoder.param_groups:\n",
    "            for param in group['params']:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # we reset the discriminator that has been affected by the decoder's loss\n",
    "        optimizer_discriminator.zero_grad()\n",
    "        loss_discriminator.backward()\n",
    "\n",
    "        optimizer_encoder.step()\n",
    "        if self.prev_disc_loss is None or self.prev_disc_loss < self.generator_too_good:\n",
    "            optimizer_decoder.step()\n",
    "            self.log('train/active/generator', 1, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "        else:\n",
    "            self.log('train/active/generator', 0, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "        if self.prev_disc_loss is None or self.prev_disc_loss > self.discriminator_too_good:\n",
    "            optimizer_discriminator.step()\n",
    "            self.log('train/active/discriminator', 1, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "        else:\n",
    "            self.log('train/active/discriminator', 0, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        self.prev_disc_loss = loss_discriminator.detach()\n",
    "\n",
    "        # resettings things to thier normal states\n",
    "        for group in optimizer_encoder.param_groups + optimizer_decoder.param_groups:\n",
    "            for param in group['params']:\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.log('train/loss_encoder', loss_encoder, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_decoder', loss_decoder, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_discriminator', loss_discriminator, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        self.log('train/loss_reconstruction', reconstruction_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_reconstruction', kl_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_decoder_reconstructed', loss_decoder_reconstructed, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_decoder_samples', loss_decoder_samples, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_discriminator_samples', loss_discriminator_samples, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_discriminator_original', loss_discriminator_original, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_discriminator_reconstructed', loss_discriminator_reconstructed, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_reconstruction', reconstruction_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        tensorboard = self.logger.experiment\n",
    "        tensorboard.add_histogram(\"train_mean\", mean, self.trainer.num_training_batches * self.current_epoch + batch_idx)\n",
    "        tensorboard.add_histogram(\"train_logvar\", logvar, self.trainer.num_training_batches * self.current_epoch + batch_idx)\n",
    "\n",
    "        tensorboard.add_histogram(\n",
    "            \"pred_originals\",\n",
    "            F.sigmoid(logits_originals),\n",
    "            self.trainer.num_training_batches * self.current_epoch + batch_idx\n",
    "        )\n",
    "        tensorboard.add_histogram(\n",
    "            \"pred_reconstructed\",\n",
    "            F.sigmoid(logits_reconstructed),\n",
    "            self.trainer.num_training_batches * self.current_epoch + batch_idx\n",
    "        )\n",
    "        tensorboard.add_histogram(\n",
    "            \"pred_sampled\",\n",
    "            F.sigmoid(logits_sampled),\n",
    "            self.trainer.num_training_batches* self.current_epoch + batch_idx\n",
    "        )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, birdname, file = batch\n",
    "        reconstructed, sampled, mean, logvar, logits_originals, logits_reconstructed, logits_sampled = self(x)\n",
    "\n",
    "        (loss_encoder,\n",
    "         loss_decoder,\n",
    "         loss_discriminator,\n",
    "         reconstruction_loss,\n",
    "         kl_loss,\n",
    "         loss_decoder_reconstructed,\n",
    "         loss_decoder_samples,\n",
    "         loss_discriminator_samples,\n",
    "         loss_discriminator_original,\n",
    "         loss_discriminator_reconstructed\n",
    "        ) = self.criterion(\n",
    "            reconstructed,\n",
    "            x,\n",
    "            mean,\n",
    "            logvar,\n",
    "                logits_sampled,\n",
    "            logits_reconstructed,\n",
    "            logits_originals\n",
    "        )\n",
    "\n",
    "        self.log('validation/loss_encoder', loss_encoder, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_decoder', loss_decoder, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_discriminator', loss_discriminator, on_epoch=True, on_step=False, batch_size=x.shape[0])\n",
    "\n",
    "        self.log('validation/loss_reconstruction', reconstruction_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_reconstruction', kl_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_decoder_reconstructed', loss_decoder_reconstructed, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_decoder_samples', loss_decoder_samples, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_discriminator_samples', loss_discriminator_samples, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_discriminator_original', loss_discriminator_original, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_discriminator_reconstructed', loss_discriminator_reconstructed, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_reconstruction', reconstruction_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        tensorboard = self.logger.experiment\n",
    "        tensorboard.add_histogram(\"val_mean\", mean, self.trainer.num_val_batches[0] * self.current_epoch + batch_idx)\n",
    "        tensorboard.add_histogram(\"val_logvar\", logvar, self.trainer.num_val_batches[0] * self.current_epoch + batch_idx)\n",
    "\n",
    "        tensorboard.add_histogram(\n",
    "            \"validation/pred_originals\",\n",
    "            F.sigmoid(logits_originals),\n",
    "            self.trainer.num_val_batches[0] * self.current_epoch + batch_idx\n",
    "        )\n",
    "        tensorboard.add_histogram(\n",
    "            \"validation/pred_reconstructed\",\n",
    "            F.sigmoid(logits_reconstructed),\n",
    "            self.trainer.num_val_batches[0] * self.current_epoch + batch_idx\n",
    "        )\n",
    "        tensorboard.add_histogram(\n",
    "            \"validation/pred_sampled\",\n",
    "            F.sigmoid(logits_sampled),\n",
    "            self.trainer.num_val_batches[0] * self.current_epoch + batch_idx\n",
    "        )\n",
    "\n",
    "    def generate_specs(self, n=None):\n",
    "        if n is None:\n",
    "            n = self.trainer.datamodule.batch_size\n",
    "        mean = torch.zeros([n, self.hidden_size]).to(self.device)\n",
    "        logvar = torch.zeros([n, self.hidden_size]).to(self.device)\n",
    "        z = self.sample(mean, logvar, seed=0)\n",
    "        specs = self.decode(z)\n",
    "\n",
    "        return self.trainer.datamodule.denormalize(specs[:, 0])\n",
    "\n",
    "    def spec_to_img(self, spec):\n",
    "        fig, ax = plt.subplots()\n",
    "        img = librosa.display.specshow(spec, x_axis='time', y_axis='mel', sr=32000, ax=ax)\n",
    "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "        ax.set(title='Mel-frequency spectrogram')\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png')\n",
    "        buffer.seek(0)\n",
    "\n",
    "        plt.close(fig)\n",
    "        image = Image.open(buffer)\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1)  # Convert to tensor and adjust dimensions\n",
    "\n",
    "        return image_tensor\n",
    "\n",
    "    def reconstruction_to_img(self, original_spec, reconstructed_spec):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # Create subplots with 1 row and 2 columns\n",
    "        img1 = librosa.display.specshow(original_spec, x_axis='time', y_axis='mel', sr=32000, ax=axs[0])\n",
    "        fig.colorbar(img1, ax=axs[0], format='%+2.0f dB')\n",
    "        axs[0].set(title='Mel-frequency spectrogram - original')\n",
    "\n",
    "        img2 = librosa.display.specshow(reconstructed_spec, x_axis='time', y_axis='mel', sr=32000, ax=axs[1])\n",
    "        fig.colorbar(img2, ax=axs[1], format='%+2.0f dB')\n",
    "        axs[1].set(title='Mel-frequency spectrogram - reconstructed')\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png')\n",
    "        buffer.seek(0)\n",
    "\n",
    "        plt.close(fig)\n",
    "        image = Image.open(buffer)\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1)  # Convert to tensor and adjust dimensions\n",
    "\n",
    "        return image_tensor\n",
    "\n",
    "    def reconstruct(self, batch):\n",
    "        output, _, _, _, _, _, _ = self(batch)\n",
    "        return output\n",
    "\n",
    "    def on_validation_end(self):\n",
    "        tensorboard = self.logger.experiment\n",
    "\n",
    "        specs = self.generate_specs(self.generate_on_epoch)\n",
    "        for i, spec in enumerate(specs):\n",
    "            tensorboard.add_image(f\"generated_spectrogram_{i}\", self.spec_to_img(spec), self.global_step)\n",
    "            audio = librosa.feature.inverse.mel_to_audio(spec)\n",
    "            tensorboard.add_audio(f\"generated_audio_{i}\", audio, self.global_step, 32000)\n",
    "\n",
    "        reconstruction_dataloader = self.trainer.datamodule.illustration_dataloader(self.reconstruct_on_epoch)\n",
    "        originals, birdnames, files = next(iter(reconstruction_dataloader))\n",
    "        originals = originals.to(self.device)\n",
    "        reconstructed = self.reconstruct(originals)\n",
    "        originals = self.trainer.datamodule.denormalize(originals)[:, 0]\n",
    "        reconstructed = self.trainer.datamodule.denormalize(reconstructed)[:, 0]\n",
    "        for i, (original_spec, reconstructed_spec) in enumerate(zip(originals, reconstructed)):\n",
    "            tensorboard.add_image(\n",
    "                f\"reconstructed_spectrogram_{i}\",\n",
    "                self.reconstruction_to_img(original_spec, reconstructed_spec),\n",
    "                self.global_step\n",
    "            )\n",
    "            original_audio = librosa.feature.inverse.mel_to_audio(original_spec)\n",
    "            reconstructed_audio = librosa.feature.inverse.mel_to_audio(reconstructed_spec)\n",
    "            tensorboard.add_audio(f\"original_audio_{i}\", original_audio, self.global_step, 32000)\n",
    "            tensorboard.add_audio(f\"reconstructed_audio_{i}\", reconstructed_audio, self.global_step, 32000)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        for scheduler in self.lr_schedulers():\n",
    "            scheduler.step()\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.logger.log_hyperparams(self.hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a7b366e-9fe7-4211-bf52-44663814c731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T23:03:10.544010Z",
     "iopub.status.busy": "2024-03-14T23:03:10.543742Z",
     "iopub.status.idle": "2024-03-14T23:18:51.175796Z",
     "shell.execute_reply": "2024-03-14T23:18:51.175152Z",
     "shell.execute_reply.started": "2024-03-14T23:03:10.543983Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae849abae1f437ba187346b41e3767d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3775013/2580557347.py:17: FutureWarning: Pass orig_sr=32000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  audio = librosa.resample(audio, sample_rate, 16000)\n",
      "/tmp/ipykernel_3775013/2580557347.py:17: FutureWarning: Pass orig_sr=32000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  audio = librosa.resample(audio, sample_rate, 16000)\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "pl.seed_everything(seed, workers=True)\n",
    "\n",
    "model = VAEGAN.load_from_checkpoint(\"vaegan/lightning_logs/version_23/checkpoints/epoch=99-step=25430.ckpt\")\n",
    "mean = torch.zeros([1024, model.hidden_size]).to(model.device)\n",
    "logvar = torch.zeros([1024, model.hidden_size]).to(model.device)\n",
    "z = model.sample(mean, logvar, seed=0)\n",
    "specs = model.decode(z)\n",
    "ds = BirdClefDataset(root_dir='train_audio')\n",
    "sample_rate = 32000\n",
    "for i, mel_spec in enumerate(tqdm(specs)):\n",
    "    filename = f\"{i:}.wav\"\n",
    "    if os.path.isfile(f\"vaegan_inference/{filename}\"):\n",
    "        continue\n",
    "    mel_spec = ds.denormalize(mel_spec)\n",
    "    audio = librosa.feature.inverse.mel_to_audio(mel_spec)\n",
    "    audio = librosa.resample(audio, sample_rate, 16000)\n",
    "    sf.write(f\"vaegan_inference/{filename}\", audio.T, 16000, format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795db1e-a97a-4c9a-9819-789097969ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aquatk",
   "language": "python",
   "name": "aquatk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
