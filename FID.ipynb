{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9ff7ca-7410-4e5d-aec3-ee0c470ae250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T23:19:41.870816Z",
     "iopub.status.busy": "2024-03-13T23:19:41.870138Z",
     "iopub.status.idle": "2024-03-13T23:19:59.110873Z",
     "shell.execute_reply": "2024-03-13T23:19:59.109514Z",
     "shell.execute_reply.started": "2024-03-13T23:19:41.870769Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 00:19:53.162187: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-14 00:19:53.162232: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-14 00:19:53.164508: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-14 00:19:53.179327: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Audio\n",
    "from PIL import Image\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "\n",
    "\n",
    "from aquatk.embedding_extractors import VGGish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29643a79-d117-48c6-8cde-bc9aef1bda77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T23:19:59.114652Z",
     "iopub.status.busy": "2024-03-13T23:19:59.113870Z",
     "iopub.status.idle": "2024-03-13T23:19:59.143742Z",
     "shell.execute_reply": "2024-03-13T23:19:59.142299Z",
     "shell.execute_reply.started": "2024-03-13T23:19:59.114608Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        bird_name=None,\n",
    "        transform=None,\n",
    "        num_samples=65_500,\n",
    "        min_db=-80,\n",
    "        max_db=0,\n",
    "        cache=True\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "\n",
    "        if cache:\n",
    "            self.cache_dir = os.path.join(os.path.dirname(os.path.abspath(self.root_dir)), 'cache')\n",
    "            os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        else:\n",
    "            self.cache_dir = None\n",
    "        self.bird_folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "        if bird_name is not None:\n",
    "            self.bird_folders = [bird_name]\n",
    "\n",
    "        self.audio_files = []\n",
    "\n",
    "        for bird_folder in self.bird_folders:\n",
    "            bird_path = os.path.join(root_dir, bird_folder)\n",
    "            audio_files = [os.path.join(bird_path, file) for file in os.listdir(bird_path) if file.endswith('.ogg')]\n",
    "            self.audio_files.extend(audio_files)\n",
    "\n",
    "    def get_spec(self, audio_path):\n",
    "        waveform, sample_rate = librosa.load(audio_path, sr=None, mono=True)\n",
    "\n",
    "        if len(waveform) < self.num_samples:\n",
    "            pad_amount = self.num_samples - len(waveform)\n",
    "            waveform = np.pad(waveform, (0, pad_amount))\n",
    "        else:\n",
    "            waveform = waveform[:self.num_samples]\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=waveform, sr=sample_rate)\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        return mel_spec\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x- self.min_db) / (self.max_db - self.min_db)\n",
    "\n",
    "    def denormalize(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.cpu().detach().numpy()\n",
    "\n",
    "        flattened_array = x.reshape((x.shape[0], -1))\n",
    "\n",
    "        min_batch_values = flattened_array.min(axis=-1, keepdims=True)\n",
    "        max_batch_values = flattened_array.max(axis=-1, keepdims=True)\n",
    "\n",
    "        normalized_array = self.min_db + ((flattened_array - min_batch_values) / (max_batch_values - min_batch_values)) * (self.max_db - self.min_db)\n",
    "\n",
    "        normalized_batch = normalized_array.reshape(x.shape)\n",
    "\n",
    "        return normalized_batch\n",
    "\n",
    "    def cache_all(self):\n",
    "        self.cache = True\n",
    "        for idx in range(len(self)):\n",
    "            self.__getitem__(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "\n",
    "        if self.cache_dir is not None:\n",
    "            cache_filename = f\"{os.path.basename(audio_path)}_{self.num_samples}.npy\"\n",
    "            cache_path = os.path.join(self.cache_dir, cache_filename)\n",
    "\n",
    "        if self.cache_dir is not None and os.path.isfile(cache_path):\n",
    "            try:\n",
    "                mel_spec = np.load(cache_path)\n",
    "            except Exception as e:\n",
    "                raise IOError(f\"Failed to read file: {cache_path}. Error: {e}\")\n",
    "        else:\n",
    "            mel_spec = self.get_spec(audio_path)\n",
    "\n",
    "            # Normalize mel_spec\n",
    "            mel_spec = self.normalize(mel_spec)\n",
    "            mel_spec = np.expand_dims(mel_spec, axis=0)\n",
    "\n",
    "            # Save mel spectrogram to cache\n",
    "            if self.cache_dir is not None:\n",
    "                np.save(cache_path, mel_spec)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            mel_spec = self.transform(mel_spec)\n",
    "\n",
    "        folder, filename = os.path.split(audio_path)\n",
    "        basedir, bird = os.path.split(folder)\n",
    "\n",
    "        return mel_spec, bird, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e8fa23-2691-445a-8ca2-8caba122de27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T23:19:59.145992Z",
     "iopub.status.busy": "2024-03-13T23:19:59.145439Z",
     "iopub.status.idle": "2024-03-13T23:19:59.174042Z",
     "shell.execute_reply": "2024-03-13T23:19:59.172299Z",
     "shell.execute_reply.started": "2024-03-13T23:19:59.145949Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 batch_size=64,\n",
    "                 validation_split=0.2,\n",
    "                 num_workers=10,\n",
    "                 bird_name=None,\n",
    "                 transform=None,\n",
    "                 num_samples=65_500,\n",
    "                 min_db=-80,\n",
    "                 max_db=0,\n",
    "                 cache=True,\n",
    "                 seed=0\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.validation_split = validation_split\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "        self.cache = cache\n",
    "        self.seed = seed\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = BirdClefDataset(self.root_dir, bird_name=self.bird_name)\n",
    "        self.normalize = dataset.normalize\n",
    "        self.denormalize = dataset.denormalize\n",
    "        if stage == 'fit' or stage is None:\n",
    "            train_dataset, validation_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                              (1 - self.validation_split, self.validation_split),\n",
    "                                                                             torch.Generator().manual_seed(self.seed))\n",
    "            self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "            self.validation_loader = DataLoader(validation_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.validation_loader\n",
    "\n",
    "    def illustration_dataloader(self, batch_size):\n",
    "        illustrative_dataset = self.validation_loader.dataset\n",
    "        illustrative_loader = DataLoader(illustrative_dataset, batch_size=batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        return illustrative_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659f12ae-13d7-46e5-802e-07e0bbb0d3f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T23:19:59.176018Z",
     "iopub.status.busy": "2024-03-13T23:19:59.175502Z",
     "iopub.status.idle": "2024-03-13T23:19:59.247958Z",
     "shell.execute_reply": "2024-03-13T23:19:59.246337Z",
     "shell.execute_reply.started": "2024-03-13T23:19:59.175978Z"
    }
   },
   "outputs": [],
   "source": [
    "root_directory = 'train_audio'\n",
    "data_module = BirdClefDataModule(root_directory, batch_size=128)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb36a58-b152-4ae6-9426-c8b3eb885f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T23:24:14.213644Z",
     "iopub.status.busy": "2024-03-13T23:24:14.212020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a904fe0b597d4dde8f1b9078702051a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3986071/3248713047.py:10: FutureWarning: Pass orig_sr=32000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  audio = librosa.resample(audio, sample_rate, 16000)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = data_module.validation_loader.dataset\n",
    "ds = BirdClefDataset(root_dir='train_audio')\n",
    "sample_rate = 32000\n",
    "for mel_spec, bird, filename in tqdm(val_dataset):\n",
    "    filename = filename[:-3] + \"wav\"\n",
    "    if os.path.isfile(f\"validation_audio/{filename}\"):\n",
    "        continue\n",
    "    mel_spec = ds.denormalize(mel_spec)\n",
    "    audio = librosa.feature.inverse.mel_to_audio(mel_spec)\n",
    "    audio = librosa.resample(audio, sample_rate, 16000)\n",
    "    sf.write(f\"validation_audio/{filename}\", audio.T, 16000, format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12501d26-4bdc-4396-8442-3845142d28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"validation_embeddings.npy\"):\n",
    "    vggish_extractor = VGGish(checkpoint_path=\"vggish_model.ckpt\", pca_params_path=\"vggish_pca_params.npz\")\n",
    "    validation_embeddings = vggish_extractor.get_embeddings(\"validation_audio\")\n",
    "    with open('validation_embeddings.npy', 'wb') as f:\n",
    "        np.save(f, validation_embeddings)\n",
    "    vggish_extractor.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282ac01-c808-404d-b595-74909422c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_module.train_loader.dataset\n",
    "ds = BirdClefDataset(root_dir='train_audio')\n",
    "sample_rate = 32000\n",
    "for mel_spec, bird, filename in tqdm(val_dataset):\n",
    "    filename = filename[:-3] + \"wav\"\n",
    "    if os.path.isfile(f\"train_audio_processed/{filename}\"):\n",
    "        continue\n",
    "    mel_spec = ds.denormalize(mel_spec)\n",
    "    audio = librosa.feature.inverse.mel_to_audio(mel_spec)\n",
    "    audio = librosa.resample(audio, sample_rate, 16000)\n",
    "    sf.write(f\"train_audio_processed/{filename}\", audio.T, 16000, format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb6aee-9b20-4d51-8ee1-002fc6e59763",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"train_embeddings.npy\"):\n",
    "    vggish_extractor = VGGish(checkpoint_path=\"vggish_model.ckpt\", pca_params_path=\"vggish_pca_params.npz\")\n",
    "    train_embeddings = vggish_extractor.get_embeddings(\"train_audio_processed\")\n",
    "    with open('train_embeddings.npy', 'wb') as f:\n",
    "        np.save(f, validation_embeddings)\n",
    "    vggish_extractor.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6575685-5735-4e74-ade6-37adb82b3b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aquatk.metrics.frechet_distance import frechet_audio_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f2c66-7c5d-4153-830d-94aebf908cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "frechet_audio_distance(train_embeddings.reshape((-1, 128), validation_embeddings.reshape((-1, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898dbaf7-9b37-43c1-929b-f1635f4c3bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aquatk",
   "language": "python",
   "name": "aquatk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
