{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9ff7ca-7410-4e5d-aec3-ee0c470ae250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:48:11.546957Z",
     "iopub.status.busy": "2024-03-14T22:48:11.546645Z",
     "iopub.status.idle": "2024-03-14T22:48:15.969274Z",
     "shell.execute_reply": "2024-03-14T22:48:15.968676Z",
     "shell.execute_reply.started": "2024-03-14T22:48:11.546928Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Audio\n",
    "from PIL import Image\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch import loggers as pl_loggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ac407-8e06-438f-a051-37cfa6ce8c36",
   "metadata": {},
   "source": [
    "## Dataset embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29643a79-d117-48c6-8cde-bc9aef1bda77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:48:15.971118Z",
     "iopub.status.busy": "2024-03-14T22:48:15.970797Z",
     "iopub.status.idle": "2024-03-14T22:48:15.983206Z",
     "shell.execute_reply": "2024-03-14T22:48:15.982583Z",
     "shell.execute_reply.started": "2024-03-14T22:48:15.971095Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        bird_name=None,\n",
    "        transform=None,\n",
    "        num_samples=65_500,\n",
    "        min_db=-80,\n",
    "        max_db=0,\n",
    "        cache=True\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "\n",
    "        if cache:\n",
    "            self.cache_dir = self.root_dir + \"_cache\"\n",
    "            os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        else:\n",
    "            self.cache_dir = None\n",
    "        self.bird_folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "        if bird_name is not None:\n",
    "            self.bird_folders = [bird_name]\n",
    "\n",
    "        self.audio_files = []\n",
    "\n",
    "        for bird_folder in self.bird_folders:\n",
    "            bird_path = os.path.join(root_dir, bird_folder)\n",
    "            audio_files = [os.path.join(bird_path, file) for file in os.listdir(bird_path) if file.endswith('.ogg')]\n",
    "            self.audio_files.extend(audio_files)\n",
    "\n",
    "    def get_spec(self, audio_path):\n",
    "        waveform, sample_rate = librosa.load(audio_path, sr=None, mono=True)\n",
    "\n",
    "        if len(waveform) < self.num_samples:\n",
    "            pad_amount = self.num_samples - len(waveform)\n",
    "            waveform = np.pad(waveform, (0, pad_amount))\n",
    "        else:\n",
    "            waveform = waveform[:self.num_samples]\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=waveform, sr=sample_rate)\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        return mel_spec\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x- self.min_db) / (self.max_db - self.min_db)\n",
    "\n",
    "    def denormalize(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.cpu().detach().numpy()\n",
    "\n",
    "        flattened_array = x.reshape((x.shape[0], -1))\n",
    "\n",
    "        min_batch_values = flattened_array.min(axis=-1, keepdims=True)\n",
    "        max_batch_values = flattened_array.max(axis=-1, keepdims=True)\n",
    "\n",
    "        normalized_array = self.min_db + ((flattened_array - min_batch_values) / (max_batch_values - min_batch_values)) * (self.max_db - self.min_db)\n",
    "\n",
    "        normalized_batch = normalized_array.reshape(x.shape)\n",
    "\n",
    "        return normalized_batch\n",
    "\n",
    "    def cache_all(self):\n",
    "        self.cache = True\n",
    "        for idx in range(len(self)):\n",
    "            self.__getitem__(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "\n",
    "        if self.cache_dir is not None:\n",
    "            cache_filename = f\"{os.path.basename(audio_path)}_{self.num_samples}.npy\"\n",
    "            cache_path = os.path.join(self.cache_dir, cache_filename)\n",
    "\n",
    "        if self.cache_dir is not None and os.path.isfile(cache_path):\n",
    "            try:\n",
    "                mel_spec = np.load(cache_path)\n",
    "            except Exception as e:\n",
    "                raise IOError(f\"Failed to read file: {cache_path}. Error: {e}\")\n",
    "        else:\n",
    "            mel_spec = self.get_spec(audio_path)\n",
    "\n",
    "            # Normalize mel_spec\n",
    "            mel_spec = self.normalize(mel_spec)\n",
    "            mel_spec = np.expand_dims(mel_spec, axis=0)\n",
    "\n",
    "            # Save mel spectrogram to cache\n",
    "            if self.cache_dir is not None:\n",
    "                np.save(cache_path, mel_spec)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            mel_spec = self.transform(mel_spec)\n",
    "\n",
    "        folder, filename = os.path.split(audio_path)\n",
    "        basedir, bird = os.path.split(folder)\n",
    "\n",
    "        return mel_spec, bird, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e8fa23-2691-445a-8ca2-8caba122de27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:48:15.984197Z",
     "iopub.status.busy": "2024-03-14T22:48:15.983985Z",
     "iopub.status.idle": "2024-03-14T22:48:16.006430Z",
     "shell.execute_reply": "2024-03-14T22:48:16.005871Z",
     "shell.execute_reply.started": "2024-03-14T22:48:15.984180Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 batch_size=64,\n",
    "                 validation_split=0.2,\n",
    "                 num_workers=10,\n",
    "                 bird_name=None,\n",
    "                 transform=None,\n",
    "                 num_samples=65_500,\n",
    "                 min_db=-80,\n",
    "                 max_db=0,\n",
    "                 cache=True,\n",
    "                 seed=0\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.validation_split = validation_split\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "        self.cache = cache\n",
    "        self.seed = seed\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = BirdClefDataset(self.root_dir, bird_name=self.bird_name)\n",
    "        self.normalize = dataset.normalize\n",
    "        self.denormalize = dataset.denormalize\n",
    "        if stage == 'fit' or stage is None:\n",
    "            train_dataset, validation_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                              (1 - self.validation_split, self.validation_split),\n",
    "                                                                             torch.Generator().manual_seed(self.seed))\n",
    "            self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "            self.validation_loader = DataLoader(validation_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.validation_loader\n",
    "\n",
    "    def illustration_dataloader(self, batch_size):\n",
    "        illustrative_dataset = self.validation_loader.dataset\n",
    "        illustrative_loader = DataLoader(illustrative_dataset, batch_size=batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        return illustrative_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659f12ae-13d7-46e5-802e-07e0bbb0d3f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:48:16.007368Z",
     "iopub.status.busy": "2024-03-14T22:48:16.007160Z",
     "iopub.status.idle": "2024-03-14T22:48:16.039590Z",
     "shell.execute_reply": "2024-03-14T22:48:16.038856Z",
     "shell.execute_reply.started": "2024-03-14T22:48:16.007346Z"
    }
   },
   "outputs": [],
   "source": [
    "root_directory = 'train_audio'\n",
    "data_module = BirdClefDataModule(root_directory, batch_size=128)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcba302-7e86-43cd-88b8-498171c9faaf",
   "metadata": {},
   "source": [
    "## VAE inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e4f3dc-3199-4436-b448-103d2f69bf18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:48:16.040534Z",
     "iopub.status.busy": "2024-03-14T22:48:16.040322Z",
     "iopub.status.idle": "2024-03-14T22:48:16.045243Z",
     "shell.execute_reply": "2024-03-14T22:48:16.044570Z",
     "shell.execute_reply.started": "2024-03-14T22:48:16.040512Z"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def __init__(self, size=16, channels=128):\n",
    "        super(UnFlatten, self).__init__()\n",
    "        self.size = size\n",
    "        self.channels = channels\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), self.channels, self.size, self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba40048-eb2d-4d37-9c93-a2519f8f90fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:48:16.046248Z",
     "iopub.status.busy": "2024-03-14T22:48:16.046014Z",
     "iopub.status.idle": "2024-03-14T22:48:16.054215Z",
     "shell.execute_reply": "2024-03-14T22:48:16.053618Z",
     "shell.execute_reply.started": "2024-03-14T22:48:16.046223Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAELoss(torch.nn.modules.loss._Loss):\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, beta=1.0, size_average=None, reduce=None, reduction: str = 'mean', reconstruction_loss=F.mse_loss):\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "        self.beta = beta\n",
    "        self.reconstruction_loss = reconstruction_loss\n",
    "\n",
    "    def forward(self, recon_x, x, mean, logvar):\n",
    "        reconstruction_loss = self.reconstruction_loss(recon_x, x, reduction=self.reduction)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp(), dim=1)\n",
    "        if self.reduction != 'none':\n",
    "            kl_loss = torch.mean(kl_loss) if self.reduction == 'mean' else torch.sum(kl_loss)\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        return total_loss, reconstruction_loss, kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2019ebe6-7141-46cf-a349-08b63f6ef869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:48:16.056195Z",
     "iopub.status.busy": "2024-03-14T22:48:16.055939Z",
     "iopub.status.idle": "2024-03-14T22:48:16.081314Z",
     "shell.execute_reply": "2024-03-14T22:48:16.080558Z",
     "shell.execute_reply.started": "2024-03-14T22:48:16.056172Z"
    }
   },
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=1,\n",
    "        img_size=256,\n",
    "        hidden_size=4096,\n",
    "        layers=[16, 32, 64],\n",
    "        learning_rate=0.001,\n",
    "        beta=1e-3,\n",
    "        activation=nn.ReLU,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        reconstruction_loss=F.mse_loss,\n",
    "        generate_on_epoch=4,\n",
    "        reconstruct_on_epoch=4,\n",
    "        seed=0\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "\n",
    "        if img_size % (2**len(layers)) != 0:\n",
    "            raise ValueError(\"An image of size {image_size} with {len(layers)} layers won't be reconstructed with the correct size\")\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = self.build_encoder(input_channels, layers, activation)\n",
    "        last_conv_size = (img_size // (2**(len(layers)+1)))**2 * layers[-1]\n",
    "        self.mean_layer = nn.Linear(last_conv_size, self.hidden_size)\n",
    "        self.logvar_layer = nn.Linear(last_conv_size, self.hidden_size)\n",
    "        self.decoder = self.build_decoder(input_channels, layers, img_size, last_conv_size, activation)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optim = optimizer\n",
    "        self.criterion = VAELoss(beta=beta, reconstruction_loss=reconstruction_loss)\n",
    "        self.generate_on_epoch = generate_on_epoch\n",
    "        self.reconstruct_on_epoch = reconstruct_on_epoch\n",
    "        self.seed = seed\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def build_encoder(self, input_channels, channels_list, activation):\n",
    "        layers = []\n",
    "        in_channels = input_channels\n",
    "        for out_channels in channels_list:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(activation())\n",
    "            #layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "            #layers.append(activation())\n",
    "            #layers.append(nn.MaxPool2d(2))\n",
    "            in_channels = out_channels\n",
    "        layers.append(Flatten())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def build_decoder(self, input_channels, channels_list, img_size, last_conv_size, activation):\n",
    "        layers = [\n",
    "            nn.Linear(self.hidden_size, last_conv_size),\n",
    "            UnFlatten(img_size // (2**(len(channels_list)+1)), channels_list[-1])\n",
    "        ]\n",
    "        for in_channels, out_channels in zip(channels_list[::-1], channels_list[-2::-1]+[input_channels]):\n",
    "            layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "            layers.append(activation())\n",
    "            #layers.append(nn.Upsample(scale_factor=2, mode=\"bilinear\"))\n",
    "            #layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "            #layers.append(activation())\n",
    "        layers.pop()\n",
    "        layers.append(nn.Sigmoid())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def sample(self, mean, logvar, seed=None):\n",
    "        std = logvar.exp() #torch.exp(0.5 * logvar)\n",
    "        if seed is None:\n",
    "            epsilon = torch.randn_like(std)\n",
    "        else:\n",
    "            gen = torch.Generator(device=self.device).manual_seed(seed)\n",
    "            epsilon = torch.empty_like(std).normal_(generator=gen)\n",
    "        z = mean + std*epsilon\n",
    "        return z\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.sample(mean, logvar)\n",
    "        x = self.decode(z)\n",
    "        return x, mean, logvar, z\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optim(self.parameters(), lr=self.learning_rate)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, threshold=1e-6, cooldown=50, min_lr=1e-6),\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1,\n",
    "                \"interval\": \"epoch\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, birdname, file = batch\n",
    "        outputs, mean, logvar, z = self(x)\n",
    "        loss, reconstruction_loss, kl_loss = self.criterion(outputs, x, mean, logvar)\n",
    "        self.log('train_loss', loss, on_epoch=True, on_step=True, prog_bar=True, batch_size=x.shape[0])\n",
    "        self.log('train_reconstruction', reconstruction_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train_kl', kl_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        tensorboard = self.logger.experiment\n",
    "        tensorboard.add_histogram(\"train_z\", z, self.global_step)\n",
    "        tensorboard.add_histogram(\"train_mean\", mean, self.global_step)\n",
    "        tensorboard.add_histogram(\"train_logvar\", logvar, self.global_step)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, birdname, file = batch\n",
    "        outputs, mean, logvar, z = self(x)\n",
    "        loss, reconstruction_loss, kl_loss = self.criterion(outputs, x, mean, logvar)\n",
    "        self.log('val_loss', loss, on_epoch=True, on_step=True, prog_bar=True, batch_size=x.shape[0])\n",
    "        self.log('val_reconstruction', reconstruction_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('val_kl', kl_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        tensorboard = self.logger.experiment\n",
    "        tensorboard.add_histogram(\"val_z\", z, self.global_step)\n",
    "        tensorboard.add_histogram(\"val_mean\", mean, self.global_step)\n",
    "        tensorboard.add_histogram(\"val_logvar\", logvar, self.global_step)\n",
    "        return loss\n",
    "\n",
    "    def generate_specs(self, n=None):\n",
    "        if n is None:\n",
    "            n = self.trainer.datamodule.batch_size\n",
    "        mean = torch.zeros([n, self.hidden_size]).to(self.device)\n",
    "        logvar = torch.zeros([n, self.hidden_size]).to(self.device)\n",
    "        z = self.sample(mean, logvar, seed=0)\n",
    "        specs = self.decode(z)\n",
    "\n",
    "        return self.trainer.datamodule.denormalize(specs[:, 0])\n",
    "\n",
    "    def spec_to_img(self, spec):\n",
    "        fig, ax = plt.subplots()\n",
    "        img = librosa.display.specshow(spec, x_axis='time', y_axis='mel', sr=32000, ax=ax)\n",
    "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "        ax.set(title='Mel-frequency spectrogram')\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png')\n",
    "        buffer.seek(0)\n",
    "\n",
    "        plt.close(fig)\n",
    "        image = Image.open(buffer)\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1)  # Convert to tensor and adjust dimensions\n",
    "\n",
    "        return image_tensor\n",
    "\n",
    "    def reconstruction_to_img(self, original_spec, reconstructed_spec):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # Create subplots with 1 row and 2 columns\n",
    "        img1 = librosa.display.specshow(original_spec, x_axis='time', y_axis='mel', sr=32000, ax=axs[0])\n",
    "        fig.colorbar(img1, ax=axs[0], format='%+2.0f dB')\n",
    "        axs[0].set(title='Mel-frequency spectrogram - original')\n",
    "\n",
    "        img2 = librosa.display.specshow(reconstructed_spec, x_axis='time', y_axis='mel', sr=32000, ax=axs[1])\n",
    "        fig.colorbar(img2, ax=axs[1], format='%+2.0f dB')\n",
    "        axs[1].set(title='Mel-frequency spectrogram - reconstructed')\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png')\n",
    "        buffer.seek(0)\n",
    "\n",
    "        plt.close(fig)\n",
    "        image = Image.open(buffer)\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1)  # Convert to tensor and adjust dimensions\n",
    "\n",
    "        return image_tensor\n",
    "\n",
    "    def interpolate(self, batch1, batch2):\n",
    "        pass\n",
    "\n",
    "    def reconstruct(self, batch):\n",
    "        output, _, _, _ = self(batch)\n",
    "        return output\n",
    "\n",
    "    def on_validation_end(self):\n",
    "        tensorboard = self.logger.experiment\n",
    "\n",
    "        specs = self.generate_specs(self.generate_on_epoch)\n",
    "        for i, spec in enumerate(specs):\n",
    "            tensorboard.add_image(f\"generated_spectrogram_{i}\", self.spec_to_img(spec), self.global_step)\n",
    "            audio = librosa.feature.inverse.mel_to_audio(spec)\n",
    "            tensorboard.add_audio(f\"generated_audio_{i}\", audio, self.global_step, 32000)\n",
    "\n",
    "        reconstruction_dataloader = self.trainer.datamodule.illustration_dataloader(self.reconstruct_on_epoch)\n",
    "        originals, birdnames, files = next(iter(reconstruction_dataloader))\n",
    "        originals = originals.to(self.device)\n",
    "        reconstructed = self.reconstruct(originals)\n",
    "        originals = self.trainer.datamodule.denormalize(originals)[:, 0]\n",
    "        reconstructed = self.trainer.datamodule.denormalize(reconstructed)[:, 0]\n",
    "        for i, (original_spec, reconstructed_spec) in enumerate(zip(originals, reconstructed)):\n",
    "            tensorboard.add_image(\n",
    "                f\"reconstructed_spectrogram_{i}\",\n",
    "                self.reconstruction_to_img(original_spec, reconstructed_spec),\n",
    "                self.global_step\n",
    "            )\n",
    "            original_audio = librosa.feature.inverse.mel_to_audio(original_spec)\n",
    "            reconstructed_audio = librosa.feature.inverse.mel_to_audio(reconstructed_spec)\n",
    "            tensorboard.add_audio(f\"original_audio_{i}\", original_audio, self.global_step, 32000)\n",
    "            tensorboard.add_audio(f\"reconstructed_audio_{i}\", reconstructed_audio, self.global_step, 32000)\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.logger.log_hyperparams(self.hparams, {\"hp/lr\": self.trainer.lr_scheduler_configs[0].scheduler.optimizer.param_groups[0][\"lr\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a7b366e-9fe7-4211-bf52-44663814c731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:48:16.082434Z",
     "iopub.status.busy": "2024-03-14T22:48:16.082145Z",
     "iopub.status.idle": "2024-03-14T23:02:50.910260Z",
     "shell.execute_reply": "2024-03-14T23:02:50.909183Z",
     "shell.execute_reply.started": "2024-03-14T22:48:16.082407Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ba97561b924c99acf38ad8b7f636a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3039700/2384562877.py:17: FutureWarning: Pass orig_sr=32000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  audio = librosa.resample(audio, sample_rate, 16000)\n",
      "/tmp/ipykernel_3039700/2384562877.py:17: FutureWarning: Pass orig_sr=32000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  audio = librosa.resample(audio, sample_rate, 16000)\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "pl.seed_everything(seed, workers=True)\n",
    "\n",
    "model = VariationalAutoEncoder.load_from_checkpoint(\"vae/lightning_logs/version_2/checkpoints/epoch=99-step=10600.ckpt\")\n",
    "mean = torch.zeros([1024, model.hidden_size]).to(model.device)\n",
    "logvar = torch.zeros([1024, model.hidden_size]).to(model.device)\n",
    "z = model.sample(mean, logvar, seed=0)\n",
    "specs = model.decode(z)\n",
    "ds = BirdClefDataset(root_dir='train_audio')\n",
    "sample_rate = 32000\n",
    "for i, mel_spec in enumerate(tqdm(specs)):\n",
    "    filename = f\"{i:}.wav\"\n",
    "    if os.path.isfile(f\"vae_inference/{filename}\"):\n",
    "        continue\n",
    "    mel_spec = ds.denormalize(mel_spec)\n",
    "    audio = librosa.feature.inverse.mel_to_audio(mel_spec)\n",
    "    audio = librosa.resample(audio, sample_rate, 16000)\n",
    "    sf.write(f\"vae_inference/{filename}\", audio.T, 16000, format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795db1e-a97a-4c9a-9819-789097969ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aquatk",
   "language": "python",
   "name": "aquatk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
