{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f383662-bcde-4d47-b238-866c89f13405",
   "metadata": {},
   "source": [
    "# Generating bird sounds with a VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab492244-39a1-4fca-9863-00f4c174b140",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607baa89-7225-453e-a54a-6618505e07d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T15:19:45.025034Z",
     "iopub.status.busy": "2024-03-15T15:19:45.024788Z",
     "iopub.status.idle": "2024-03-15T15:19:50.585012Z",
     "shell.execute_reply": "2024-03-15T15:19:50.584382Z",
     "shell.execute_reply.started": "2024-03-15T15:19:45.024992Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Audio\n",
    "from PIL import Image\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f71ba-be56-459c-a755-333a60f44bb1",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a221bf20-b88f-415e-9366-3a6e520fcb8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T15:19:50.587323Z",
     "iopub.status.busy": "2024-03-15T15:19:50.586671Z",
     "iopub.status.idle": "2024-03-15T15:19:50.601499Z",
     "shell.execute_reply": "2024-03-15T15:19:50.600819Z",
     "shell.execute_reply.started": "2024-03-15T15:19:50.587290Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        bird_name=None,\n",
    "        transform=None,\n",
    "        num_samples=65_500,\n",
    "        min_db=-80,\n",
    "        max_db=0,\n",
    "        cache=True\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "\n",
    "        if cache:\n",
    "            self.cache_dir = self.root_dir + \"_cache\"\n",
    "            os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        else:\n",
    "            self.cache_dir = None\n",
    "        self.bird_folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "        if bird_name is not None:\n",
    "            self.bird_folders = [bird_name]\n",
    "\n",
    "        self.audio_files = []\n",
    "\n",
    "        for bird_folder in self.bird_folders:\n",
    "            bird_path = os.path.join(root_dir, bird_folder)\n",
    "            audio_files = [os.path.join(bird_path, file) for file in os.listdir(bird_path) if file.endswith('.ogg')]\n",
    "            self.audio_files.extend(audio_files)\n",
    "\n",
    "    def get_spec(self, audio_path):\n",
    "        waveform, sample_rate = librosa.load(audio_path, sr=None, mono=True)\n",
    "\n",
    "        if len(waveform) < self.num_samples:\n",
    "            pad_amount = self.num_samples - len(waveform)\n",
    "            waveform = np.pad(waveform, (0, pad_amount))\n",
    "        else:\n",
    "            waveform = waveform[:self.num_samples]\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=waveform, sr=sample_rate)\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        return mel_spec\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x- self.min_db) / (self.max_db - self.min_db)\n",
    "\n",
    "    def denormalize(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.cpu().detach().numpy()\n",
    "\n",
    "        flattened_array = x.reshape((x.shape[0], -1))\n",
    "\n",
    "        min_batch_values = flattened_array.min(axis=-1, keepdims=True)\n",
    "        max_batch_values = flattened_array.max(axis=-1, keepdims=True)\n",
    "\n",
    "        normalized_array = self.min_db + ((flattened_array - min_batch_values) / (max_batch_values - min_batch_values)) * (self.max_db - self.min_db)\n",
    "\n",
    "        normalized_batch = normalized_array.reshape(x.shape)\n",
    "\n",
    "        return normalized_batch\n",
    "\n",
    "    def cache_all(self):\n",
    "        self.cache = True\n",
    "        for idx in range(len(self)):\n",
    "            self.__getitem__(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "\n",
    "        if self.cache_dir is not None:\n",
    "            cache_filename = f\"{os.path.basename(audio_path)}_{self.num_samples}.npy\"\n",
    "            cache_path = os.path.join(self.cache_dir, cache_filename)\n",
    "\n",
    "        if self.cache_dir is not None and os.path.isfile(cache_path):\n",
    "            mel_spec = np.load(cache_path)\n",
    "        else:\n",
    "            mel_spec = self.get_spec(audio_path)\n",
    "\n",
    "            # Normalize mel_spec\n",
    "            mel_spec = self.normalize(mel_spec)\n",
    "            mel_spec = np.expand_dims(mel_spec, axis=0)\n",
    "\n",
    "            # Save mel spectrogram to cache\n",
    "        if self.cache_dir is not None:\n",
    "            np.save(cache_path, mel_spec)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            mel_spec = self.transform(mel_spec)\n",
    "\n",
    "        folder, filename = os.path.split(audio_path)\n",
    "        basedir, bird = os.path.split(folder)\n",
    "\n",
    "        return mel_spec, bird, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e7f6888-92a3-47f4-a719-ec97e87c7ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T15:19:50.602705Z",
     "iopub.status.busy": "2024-03-15T15:19:50.602461Z",
     "iopub.status.idle": "2024-03-15T15:19:50.612518Z",
     "shell.execute_reply": "2024-03-15T15:19:50.611762Z",
     "shell.execute_reply.started": "2024-03-15T15:19:50.602687Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 batch_size=64,\n",
    "                 validation_split=0.2,\n",
    "                 num_workers=10,\n",
    "                 bird_name=None,\n",
    "                 transform=None,\n",
    "                 num_samples=65_500,\n",
    "                 min_db=-80,\n",
    "                 max_db=0,\n",
    "                 cache=True,\n",
    "                 seed=0\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.validation_split = validation_split\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "        self.cache = cache\n",
    "        self.seed = seed\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = BirdClefDataset(self.root_dir, bird_name=self.bird_name)\n",
    "        self.normalize = dataset.normalize\n",
    "        self.denormalize = dataset.denormalize\n",
    "        if stage == 'fit' or stage is None:\n",
    "            train_dataset, validation_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                              (1 - self.validation_split, self.validation_split),\n",
    "                                                                             torch.Generator().manual_seed(self.seed))\n",
    "            self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "            self.validation_loader = DataLoader(validation_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.validation_loader\n",
    "\n",
    "    def illustration_dataloader(self, batch_size):\n",
    "        illustrative_dataset = self.validation_loader.dataset\n",
    "        illustrative_loader = DataLoader(illustrative_dataset, batch_size=batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        return illustrative_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a05391-c206-442b-b32b-a2f42de612da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T15:19:50.613652Z",
     "iopub.status.busy": "2024-03-15T15:19:50.613381Z",
     "iopub.status.idle": "2024-03-15T15:19:50.621248Z",
     "shell.execute_reply": "2024-03-15T15:19:50.620735Z",
     "shell.execute_reply.started": "2024-03-15T15:19:50.613625Z"
    }
   },
   "outputs": [],
   "source": [
    "root_directory = 'audio_from_peak'\n",
    "data_module = BirdClefDataModule(root_directory, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb1e1e-5fa2-4d02-8bdd-99383247594e",
   "metadata": {},
   "source": [
    "## VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d169659-0b0b-4ac1-9063-b1428937d99f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T15:19:50.622017Z",
     "iopub.status.busy": "2024-03-15T15:19:50.621854Z",
     "iopub.status.idle": "2024-03-15T15:19:50.628525Z",
     "shell.execute_reply": "2024-03-15T15:19:50.627780Z",
     "shell.execute_reply.started": "2024-03-15T15:19:50.622000Z"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def __init__(self, size=16, channels=128):\n",
    "        super(UnFlatten, self).__init__()\n",
    "        self.size = size\n",
    "        self.channels = channels\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), self.channels, self.size, self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab161ae6-b928-453f-8b06-b8c5c698d2f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T15:19:50.629656Z",
     "iopub.status.busy": "2024-03-15T15:19:50.629427Z",
     "iopub.status.idle": "2024-03-15T15:19:50.635459Z",
     "shell.execute_reply": "2024-03-15T15:19:50.634752Z",
     "shell.execute_reply.started": "2024-03-15T15:19:50.629630Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAELoss(torch.nn.modules.loss._Loss):\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, beta=1.0, size_average=None, reduce=None, reduction: str = 'mean', reconstruction_loss=F.mse_loss):\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "        self.beta = beta\n",
    "        self.reconstruction_loss = reconstruction_loss\n",
    "\n",
    "    def forward(self, recon_x, x, mean, logvar):\n",
    "        reconstruction_loss = self.reconstruction_loss(recon_x, x, reduction=self.reduction)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp(), dim=1)\n",
    "        if self.reduction != 'none':\n",
    "            kl_loss = torch.mean(kl_loss) if self.reduction == 'mean' else torch.sum(kl_loss)\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        return total_loss, reconstruction_loss, kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cbe3d27-24c8-47ce-91ed-727167823d67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T15:19:50.637890Z",
     "iopub.status.busy": "2024-03-15T15:19:50.637633Z",
     "iopub.status.idle": "2024-03-15T15:19:50.665510Z",
     "shell.execute_reply": "2024-03-15T15:19:50.664783Z",
     "shell.execute_reply.started": "2024-03-15T15:19:50.637870Z"
    }
   },
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=1,\n",
    "        img_size=256,\n",
    "        hidden_size=4096,\n",
    "        layers=[16, 32, 64],\n",
    "        learning_rate=0.001,\n",
    "        lr_decay=1,\n",
    "        beta=1e-3,\n",
    "        activation=nn.ReLU,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        reconstruction_loss=F.mse_loss,\n",
    "        generate_on_epoch=4,\n",
    "        reconstruct_on_epoch=4,\n",
    "        seed=0\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "\n",
    "        if img_size % (2**len(layers)) != 0:\n",
    "            raise ValueError(\"An image of size {image_size} with {len(layers)} layers won't be reconstructed with the correct size\")\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = self.build_encoder(input_channels, layers, activation)\n",
    "        last_conv_size = (img_size // (2**(len(layers)+1)))**2 * layers[-1]\n",
    "        self.mean_layer = nn.Linear(last_conv_size, self.hidden_size)\n",
    "        self.logvar_layer = nn.Linear(last_conv_size, self.hidden_size)\n",
    "        self.decoder = self.build_decoder(input_channels, layers, img_size, last_conv_size, activation)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.optim = optimizer\n",
    "        self.criterion = VAELoss(beta=beta, reconstruction_loss=reconstruction_loss)\n",
    "        self.generate_on_epoch = generate_on_epoch\n",
    "        self.reconstruct_on_epoch = reconstruct_on_epoch\n",
    "        self.seed = seed\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def build_encoder(self, input_channels, channels_list, activation):\n",
    "        layers = []\n",
    "        in_channels = input_channels\n",
    "        for out_channels in channels_list:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(activation())\n",
    "            #layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "            #layers.append(activation())\n",
    "            #layers.append(nn.MaxPool2d(2))\n",
    "            in_channels = out_channels\n",
    "        layers.append(Flatten())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def build_decoder(self, input_channels, channels_list, img_size, last_conv_size, activation):\n",
    "        layers = [\n",
    "            nn.Linear(self.hidden_size, last_conv_size),\n",
    "            UnFlatten(img_size // (2**(len(channels_list)+1)), channels_list[-1])\n",
    "        ]\n",
    "        for in_channels, out_channels in zip(channels_list[::-1], channels_list[-2::-1]+[input_channels]):\n",
    "            layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "            layers.append(activation())\n",
    "            #layers.append(nn.Upsample(scale_factor=2, mode=\"bilinear\"))\n",
    "            #layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "            #layers.append(activation())\n",
    "        layers.pop()\n",
    "        layers.append(nn.Sigmoid())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def sample(self, mean, logvar, seed=None):\n",
    "        std = logvar.exp() #torch.exp(0.5 * logvar)\n",
    "        if seed is None:\n",
    "            epsilon = torch.randn_like(std)\n",
    "        else:\n",
    "            gen = torch.Generator(device=self.device).manual_seed(seed)\n",
    "            epsilon = torch.empty_like(std).normal_(generator=gen)\n",
    "        z = mean + std*epsilon\n",
    "        return z\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.sample(mean, logvar)\n",
    "        x = self.decode(z)\n",
    "        return x, mean, logvar, z\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optim(self.parameters(), lr=self.learning_rate)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=self.lr_decay),\n",
    "                \"frequency\": 1,\n",
    "                \"interval\": \"epoch\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, birdname, file = batch\n",
    "        outputs, mean, logvar, z = self(x)\n",
    "        loss, reconstruction_loss, kl_loss = self.criterion(outputs, x, mean, logvar)\n",
    "        self.log('train/loss', loss, on_epoch=True, on_step=True, prog_bar=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_reconstruction', reconstruction_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_kl', kl_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        tensorboard = self.logger.experiment\n",
    "        tensorboard.add_histogram(\"train/z\", z, self.global_step)\n",
    "        tensorboard.add_histogram(\"train/mean\", mean, self.global_step)\n",
    "        tensorboard.add_histogram(\"train/logvar\", logvar, self.global_step)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, birdname, file = batch\n",
    "        outputs, mean, logvar, z = self(x)\n",
    "        loss, reconstruction_loss, kl_loss = self.criterion(outputs, x, mean, logvar)\n",
    "        self.log('validation/loss', loss, on_epoch=True, on_step=True, prog_bar=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_reconstruction', reconstruction_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_kl', kl_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        tensorboard = self.logger.experiment\n",
    "        tensorboard.add_histogram(\"validation/z\", z, self.global_step)\n",
    "        tensorboard.add_histogram(\"validation/mean\", mean, self.global_step)\n",
    "        tensorboard.add_histogram(\"validation/logvar\", logvar, self.global_step)\n",
    "        return loss\n",
    "\n",
    "    def generate_specs(self, n=None):\n",
    "        if n is None:\n",
    "            n = self.trainer.datamodule.batch_size\n",
    "        mean = torch.zeros([n, self.hidden_size]).to(self.device)\n",
    "        logvar = torch.zeros([n, self.hidden_size]).to(self.device)\n",
    "        z = self.sample(mean, logvar, seed=0)\n",
    "        specs = self.decode(z)\n",
    "\n",
    "        return self.trainer.datamodule.denormalize(specs[:, 0])\n",
    "\n",
    "    def spec_to_img(self, spec):\n",
    "        fig, ax = plt.subplots()\n",
    "        img = librosa.display.specshow(spec, x_axis='time', y_axis='mel', sr=32000, ax=ax)\n",
    "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "        ax.set(title='Mel-frequency spectrogram')\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png')\n",
    "        buffer.seek(0)\n",
    "\n",
    "        plt.close(fig)\n",
    "        image = Image.open(buffer)\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1)  # Convert to tensor and adjust dimensions\n",
    "\n",
    "        return image_tensor\n",
    "\n",
    "    def reconstruction_to_img(self, original_spec, reconstructed_spec):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # Create subplots with 1 row and 2 columns\n",
    "        img1 = librosa.display.specshow(original_spec, x_axis='time', y_axis='mel', sr=32000, ax=axs[0])\n",
    "        fig.colorbar(img1, ax=axs[0], format='%+2.0f dB')\n",
    "        axs[0].set(title='Mel-frequency spectrogram - original')\n",
    "\n",
    "        img2 = librosa.display.specshow(reconstructed_spec, x_axis='time', y_axis='mel', sr=32000, ax=axs[1])\n",
    "        fig.colorbar(img2, ax=axs[1], format='%+2.0f dB')\n",
    "        axs[1].set(title='Mel-frequency spectrogram - reconstructed')\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png')\n",
    "        buffer.seek(0)\n",
    "\n",
    "        plt.close(fig)\n",
    "        image = Image.open(buffer)\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1)  # Convert to tensor and adjust dimensions\n",
    "\n",
    "        return image_tensor\n",
    "\n",
    "    def interpolate(self, batch1, batch2):\n",
    "        pass\n",
    "\n",
    "    def reconstruct(self, batch):\n",
    "        output, _, _, _ = self(batch)\n",
    "        return output\n",
    "\n",
    "    def on_validation_end(self):\n",
    "        tensorboard = self.logger.experiment\n",
    "\n",
    "        specs = self.generate_specs(self.generate_on_epoch)\n",
    "        for i, spec in enumerate(specs):\n",
    "            tensorboard.add_image(f\"generated_spectrogram_{i}\", self.spec_to_img(spec), self.global_step)\n",
    "            audio = librosa.feature.inverse.mel_to_audio(spec)\n",
    "            tensorboard.add_audio(f\"generated_audio_{i}\", audio, self.global_step, 32000)\n",
    "\n",
    "        reconstruction_dataloader = self.trainer.datamodule.illustration_dataloader(self.reconstruct_on_epoch)\n",
    "        originals, birdnames, files = next(iter(reconstruction_dataloader))\n",
    "        originals = originals.to(self.device)\n",
    "        reconstructed = self.reconstruct(originals)\n",
    "        originals = self.trainer.datamodule.denormalize(originals)[:, 0]\n",
    "        reconstructed = self.trainer.datamodule.denormalize(reconstructed)[:, 0]\n",
    "        for i, (original_spec, reconstructed_spec) in enumerate(zip(originals, reconstructed)):\n",
    "            tensorboard.add_image(\n",
    "                f\"reconstructed_spectrogram_{i}\",\n",
    "                self.reconstruction_to_img(original_spec, reconstructed_spec),\n",
    "                self.global_step\n",
    "            )\n",
    "            original_audio = librosa.feature.inverse.mel_to_audio(original_spec)\n",
    "            reconstructed_audio = librosa.feature.inverse.mel_to_audio(reconstructed_spec)\n",
    "            tensorboard.add_audio(f\"original_audio_{i}\", original_audio, self.global_step, 32000)\n",
    "            tensorboard.add_audio(f\"reconstructed_audio_{i}\", reconstructed_audio, self.global_step, 32000)\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.logger.log_hyperparams(self.hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3d60e7-0d75-4fdd-96a5-ac2fb2194352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T15:19:50.666682Z",
     "iopub.status.busy": "2024-03-15T15:19:50.666417Z",
     "iopub.status.idle": "2024-03-15T15:19:51.517471Z",
     "shell.execute_reply": "2024-03-15T15:19:51.516885Z",
     "shell.execute_reply.started": "2024-03-15T15:19:50.666662Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "pl.seed_everything(seed, workers=True)\n",
    "model = VariationalAutoEncoder(\n",
    "    input_channels=1,\n",
    "    img_size=256,\n",
    "    hidden_size=8192,\n",
    "    layers=[32, 64, 128, 256, 256],\n",
    "    learning_rate=0.001,\n",
    "    beta=5e-5,\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e185172-2f0d-41c1-9d7d-a75d9615e518",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7cdfa7-4b05-42ae-9dcd-6337b6bf751c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T15:19:51.518384Z",
     "iopub.status.busy": "2024-03-15T15:19:51.518126Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[codecarbon INFO @ 16:19:51] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 16:19:51] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 16:19:51] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 16:19:51] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 16:19:51] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 16:19:53] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\n",
      "[codecarbon INFO @ 16:19:53] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 16:19:53]   Platform system: Linux-6.6.10-76060610-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 16:19:53]   Python version: 3.10.13\n",
      "[codecarbon INFO @ 16:19:53]   CodeCarbon version: 2.2.2\n",
      "[codecarbon INFO @ 16:19:53]   Available RAM : 15.472 GB\n",
      "[codecarbon INFO @ 16:19:53]   CPU count: 12\n",
      "[codecarbon INFO @ 16:19:53]   CPU model: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\n",
      "[codecarbon INFO @ 16:19:53]   GPU count: 1\n",
      "[codecarbon INFO @ 16:19:53]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | encoder      | Sequential | 977 K \n",
      "1 | mean_layer   | Linear     | 33.6 M\n",
      "2 | logvar_layer | Linear     | 33.6 M\n",
      "3 | decoder      | Sequential | 34.5 M\n",
      "4 | criterion    | VAELoss    | 0     \n",
      "--------------------------------------------\n",
      "102 M     Trainable params\n",
      "0         Non-trainable params\n",
      "102 M     Total params\n",
      "410.557   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateo/mambaforge/envs/ml/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1699449181202/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "[codecarbon INFO @ 16:20:11] Energy consumed for RAM : 0.000024 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:20:11] Energy consumed for all GPUs : 0.000117 kWh. Total GPU Power : 27.867000000000004 W\n",
      "[codecarbon INFO @ 16:20:11] Energy consumed for all CPUs : 0.000095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:20:11] 0.000236 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46170b53fb32483787437b7d44bf2715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:20:26] Energy consumed for RAM : 0.000048 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:20:26] Energy consumed for all GPUs : 0.000332 kWh. Total GPU Power : 52.56100000000001 W\n",
      "[codecarbon INFO @ 16:20:26] Energy consumed for all CPUs : 0.000187 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:20:26] 0.000568 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:20:41] Energy consumed for RAM : 0.000072 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:20:41] Energy consumed for all GPUs : 0.000548 kWh. Total GPU Power : 51.936 W\n",
      "[codecarbon INFO @ 16:20:41] Energy consumed for all CPUs : 0.000281 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:20:41] 0.000902 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:20:56] Energy consumed for RAM : 0.000096 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:20:56] Energy consumed for all GPUs : 0.000827 kWh. Total GPU Power : 66.97 W\n",
      "[codecarbon INFO @ 16:20:56] Energy consumed for all CPUs : 0.000375 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:20:56] 0.001298 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:21:11] Energy consumed for RAM : 0.000120 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:21:11] Energy consumed for all GPUs : 0.001052 kWh. Total GPU Power : 54.146 W\n",
      "[codecarbon INFO @ 16:21:11] Energy consumed for all CPUs : 0.000469 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:21:11] 0.001641 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:21:26] Energy consumed for RAM : 0.000145 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:21:26] Energy consumed for all GPUs : 0.001185 kWh. Total GPU Power : 32.011 W\n",
      "[codecarbon INFO @ 16:21:26] Energy consumed for all CPUs : 0.000562 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:21:26] 0.001892 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:21:41] Energy consumed for RAM : 0.000169 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:21:43] Energy consumed for all GPUs : 0.001323 kWh. Total GPU Power : 31.686 W\n",
      "[codecarbon INFO @ 16:21:43] Energy consumed for all CPUs : 0.000668 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:21:43] 0.002160 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:21:56] Energy consumed for RAM : 0.000190 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:21:56] Energy consumed for all GPUs : 0.001672 kWh. Total GPU Power : 94.69800000000001 W\n",
      "[codecarbon INFO @ 16:21:56] Energy consumed for all CPUs : 0.000751 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:21:56] 0.002613 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:22:11] Energy consumed for RAM : 0.000214 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:22:11] Energy consumed for all GPUs : 0.001925 kWh. Total GPU Power : 60.905 W\n",
      "[codecarbon INFO @ 16:22:11] Energy consumed for all CPUs : 0.000845 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:22:11] 0.002984 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:22:26] Energy consumed for RAM : 0.000239 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:22:26] Energy consumed for all GPUs : 0.002219 kWh. Total GPU Power : 70.586 W\n",
      "[codecarbon INFO @ 16:22:26] Energy consumed for all CPUs : 0.000938 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:22:26] 0.003396 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:22:41] Energy consumed for RAM : 0.000263 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:22:41] Energy consumed for all GPUs : 0.002423 kWh. Total GPU Power : 48.912 W\n",
      "[codecarbon INFO @ 16:22:41] Energy consumed for all CPUs : 0.001032 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:22:41] 0.003718 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:22:56] Energy consumed for RAM : 0.000287 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:22:56] Energy consumed for all GPUs : 0.002560 kWh. Total GPU Power : 32.786 W\n",
      "[codecarbon INFO @ 16:22:56] Energy consumed for all CPUs : 0.001127 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:22:56] 0.003974 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:23:11] Energy consumed for RAM : 0.000311 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:23:11] Energy consumed for all GPUs : 0.002873 kWh. Total GPU Power : 76.392 W\n",
      "[codecarbon INFO @ 16:23:11] Energy consumed for all CPUs : 0.001219 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:23:11] 0.004403 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:23:26] Energy consumed for RAM : 0.000335 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:23:26] Energy consumed for all GPUs : 0.003102 kWh. Total GPU Power : 55.039 W\n",
      "[codecarbon INFO @ 16:23:26] Energy consumed for all CPUs : 0.001313 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:23:26] 0.004750 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:23:41] Energy consumed for RAM : 0.000359 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:23:41] Energy consumed for all GPUs : 0.003279 kWh. Total GPU Power : 42.315 W\n",
      "[codecarbon INFO @ 16:23:41] Energy consumed for all CPUs : 0.001407 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:23:41] 0.005045 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:23:56] Energy consumed for RAM : 0.000383 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:23:56] Energy consumed for all GPUs : 0.003534 kWh. Total GPU Power : 61.45 W\n",
      "[codecarbon INFO @ 16:23:56] Energy consumed for all CPUs : 0.001501 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:23:56] 0.005418 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:24:11] Energy consumed for RAM : 0.000407 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:24:11] Energy consumed for all GPUs : 0.003741 kWh. Total GPU Power : 49.566 W\n",
      "[codecarbon INFO @ 16:24:11] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:24:11] 0.005743 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:24:26] Energy consumed for RAM : 0.000432 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:24:26] Energy consumed for all GPUs : 0.003880 kWh. Total GPU Power : 33.16600000000001 W\n",
      "[codecarbon INFO @ 16:24:26] Energy consumed for all CPUs : 0.001690 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:24:26] 0.006001 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:24:41] Energy consumed for RAM : 0.000455 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:24:41] Energy consumed for all GPUs : 0.004142 kWh. Total GPU Power : 64.049 W\n",
      "[codecarbon INFO @ 16:24:41] Energy consumed for all CPUs : 0.001782 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:24:41] 0.006379 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:24:56] Energy consumed for RAM : 0.000479 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:24:56] Energy consumed for all GPUs : 0.004305 kWh. Total GPU Power : 39.13 W\n",
      "[codecarbon INFO @ 16:24:56] Energy consumed for all CPUs : 0.001875 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:24:56] 0.006660 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:25:11] Energy consumed for RAM : 0.000504 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:25:11] Energy consumed for all GPUs : 0.004560 kWh. Total GPU Power : 61.315000000000005 W\n",
      "[codecarbon INFO @ 16:25:11] Energy consumed for all CPUs : 0.001969 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:25:11] 0.007033 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:25:26] Energy consumed for RAM : 0.000528 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:25:26] Energy consumed for all GPUs : 0.004766 kWh. Total GPU Power : 49.488 W\n",
      "[codecarbon INFO @ 16:25:26] Energy consumed for all CPUs : 0.002063 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:25:26] 0.007357 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:25:41] Energy consumed for RAM : 0.000553 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:25:42] Energy consumed for all GPUs : 0.004910 kWh. Total GPU Power : 33.752 W\n",
      "[codecarbon INFO @ 16:25:42] Energy consumed for all CPUs : 0.002159 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:25:42] 0.007622 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:25:57] Energy consumed for RAM : 0.000577 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:25:57] Energy consumed for all GPUs : 0.005054 kWh. Total GPU Power : 33.208 W\n",
      "[codecarbon INFO @ 16:25:57] Energy consumed for all CPUs : 0.002258 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:25:57] 0.007888 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:26:12] Energy consumed for RAM : 0.000600 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:26:12] Energy consumed for all GPUs : 0.005219 kWh. Total GPU Power : 41.854 W\n",
      "[codecarbon INFO @ 16:26:12] Energy consumed for all CPUs : 0.002347 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:26:12] 0.008166 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:26:27] Energy consumed for RAM : 0.000624 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:26:27] Energy consumed for all GPUs : 0.005461 kWh. Total GPU Power : 58.142 W\n",
      "[codecarbon INFO @ 16:26:27] Energy consumed for all CPUs : 0.002440 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:26:27] 0.008526 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:26:42] Energy consumed for RAM : 0.000648 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:26:42] Energy consumed for all GPUs : 0.005614 kWh. Total GPU Power : 36.699 W\n",
      "[codecarbon INFO @ 16:26:42] Energy consumed for all CPUs : 0.002534 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:26:42] 0.008796 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:26:57] Energy consumed for RAM : 0.000672 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:26:57] Energy consumed for all GPUs : 0.005766 kWh. Total GPU Power : 36.517 W\n",
      "[codecarbon INFO @ 16:26:57] Energy consumed for all CPUs : 0.002628 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:26:57] 0.009066 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:27:12] Energy consumed for RAM : 0.000696 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:27:12] Energy consumed for all GPUs : 0.005908 kWh. Total GPU Power : 33.833 W\n",
      "[codecarbon INFO @ 16:27:12] Energy consumed for all CPUs : 0.002723 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:27:12] 0.009327 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:27:27] Energy consumed for RAM : 0.000720 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:27:27] Energy consumed for all GPUs : 0.006042 kWh. Total GPU Power : 32.69 W\n",
      "[codecarbon INFO @ 16:27:27] Energy consumed for all CPUs : 0.002815 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:27:27] 0.009577 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:27:42] Energy consumed for RAM : 0.000744 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:27:42] Energy consumed for all GPUs : 0.006435 kWh. Total GPU Power : 94.649 W\n",
      "[codecarbon INFO @ 16:27:42] Energy consumed for all CPUs : 0.002909 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:27:42] 0.010088 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:27:57] Energy consumed for RAM : 0.000768 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:27:57] Energy consumed for all GPUs : 0.006666 kWh. Total GPU Power : 55.367000000000004 W\n",
      "[codecarbon INFO @ 16:27:57] Energy consumed for all CPUs : 0.003002 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:27:57] 0.010436 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:28:12] Energy consumed for RAM : 0.000793 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:28:12] Energy consumed for all GPUs : 0.006972 kWh. Total GPU Power : 73.52700000000002 W\n",
      "[codecarbon INFO @ 16:28:12] Energy consumed for all CPUs : 0.003096 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:28:12] 0.010860 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:28:27] Energy consumed for RAM : 0.000817 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:28:27] Energy consumed for all GPUs : 0.007123 kWh. Total GPU Power : 36.273 W\n",
      "[codecarbon INFO @ 16:28:27] Energy consumed for all CPUs : 0.003190 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:28:27] 0.011129 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:28:42] Energy consumed for RAM : 0.000841 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:28:42] Energy consumed for all GPUs : 0.007264 kWh. Total GPU Power : 33.713 W\n",
      "[codecarbon INFO @ 16:28:42] Energy consumed for all CPUs : 0.003284 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:28:42] 0.011388 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:28:57] Energy consumed for RAM : 0.000865 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:28:57] Energy consumed for all GPUs : 0.007432 kWh. Total GPU Power : 40.461 W\n",
      "[codecarbon INFO @ 16:28:57] Energy consumed for all CPUs : 0.003377 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:28:57] 0.011674 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:29:12] Energy consumed for RAM : 0.000889 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:29:12] Energy consumed for all GPUs : 0.007649 kWh. Total GPU Power : 52.217 W\n",
      "[codecarbon INFO @ 16:29:12] Energy consumed for all CPUs : 0.003471 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:29:12] 0.012009 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:29:27] Energy consumed for RAM : 0.000913 kWh. RAM Power : 5.8020172119140625 W\n",
      "[codecarbon INFO @ 16:29:27] Energy consumed for all GPUs : 0.007843 kWh. Total GPU Power : 46.652 W\n",
      "[codecarbon INFO @ 16:29:27] Energy consumed for all CPUs : 0.003565 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:29:27] 0.012321 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"vae/\", default_hp_metric=False)\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
    "#model_chkpt = pl.callbacks.ModelCheckpoint(save_top_k=1, monitor=\"val_loss\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator='auto',\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[lr_monitor],#, model_chkpt],\n",
    "    logger=tb_logger,\n",
    "    #overfit_batches=10,\n",
    "    #enable_checkpointing=False,\n",
    "    deterministic=True\n",
    ")\n",
    "#tuner = pl.tuner.Tuner(trainer)\n",
    "#tuner.scale_batch_size(model, mode=\"binsearch\", datamodule=data_module, init_val=64)\n",
    "with EmissionsTracker() as tracker:\n",
    "    trainer.fit(model, data_module)#, ckpt_path=\"logs/lightning_logs/version_0/checkpoints/epoch=294-step=62540.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b59ac5-1378-4fb5-b7a8-d91a921bcb17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
