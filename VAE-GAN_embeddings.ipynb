{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9ff7ca-7410-4e5d-aec3-ee0c470ae250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T09:24:12.814406Z",
     "iopub.status.busy": "2024-03-14T09:24:12.813951Z",
     "iopub.status.idle": "2024-03-14T09:24:19.877321Z",
     "shell.execute_reply": "2024-03-14T09:24:19.876799Z",
     "shell.execute_reply.started": "2024-03-14T09:24:12.814374Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 10:24:17.688263: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-14 10:24:17.688289: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-14 10:24:17.689298: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-14 10:24:17.699024: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Audio\n",
    "from PIL import Image\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "\n",
    "\n",
    "from aquatk.embedding_extractors import VGGish\n",
    "from aquatk.metrics.frechet_distance import frechet_audio_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ac407-8e06-438f-a051-37cfa6ce8c36",
   "metadata": {},
   "source": [
    "## Dataset embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29643a79-d117-48c6-8cde-bc9aef1bda77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T09:24:19.878781Z",
     "iopub.status.busy": "2024-03-14T09:24:19.878458Z",
     "iopub.status.idle": "2024-03-14T09:24:19.894412Z",
     "shell.execute_reply": "2024-03-14T09:24:19.893889Z",
     "shell.execute_reply.started": "2024-03-14T09:24:19.878764Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        bird_name=None,\n",
    "        transform=None,\n",
    "        num_samples=65_500,\n",
    "        min_db=-80,\n",
    "        max_db=0,\n",
    "        cache=True\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "\n",
    "        if cache:\n",
    "            self.cache_dir = os.path.join(os.path.dirname(os.path.abspath(self.root_dir)), 'cache')\n",
    "            os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        else:\n",
    "            self.cache_dir = None\n",
    "        self.bird_folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "        if bird_name is not None:\n",
    "            self.bird_folders = [bird_name]\n",
    "\n",
    "        self.audio_files = []\n",
    "\n",
    "        for bird_folder in self.bird_folders:\n",
    "            bird_path = os.path.join(root_dir, bird_folder)\n",
    "            audio_files = [os.path.join(bird_path, file) for file in os.listdir(bird_path) if file.endswith('.ogg')]\n",
    "            self.audio_files.extend(audio_files)\n",
    "\n",
    "    def get_spec(self, audio_path):\n",
    "        waveform, sample_rate = librosa.load(audio_path, sr=None, mono=True)\n",
    "\n",
    "        if len(waveform) < self.num_samples:\n",
    "            pad_amount = self.num_samples - len(waveform)\n",
    "            waveform = np.pad(waveform, (0, pad_amount))\n",
    "        else:\n",
    "            waveform = waveform[:self.num_samples]\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=waveform, sr=sample_rate)\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        return mel_spec\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x- self.min_db) / (self.max_db - self.min_db)\n",
    "\n",
    "    def denormalize(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.cpu().detach().numpy()\n",
    "\n",
    "        flattened_array = x.reshape((x.shape[0], -1))\n",
    "\n",
    "        min_batch_values = flattened_array.min(axis=-1, keepdims=True)\n",
    "        max_batch_values = flattened_array.max(axis=-1, keepdims=True)\n",
    "\n",
    "        normalized_array = self.min_db + ((flattened_array - min_batch_values) / (max_batch_values - min_batch_values)) * (self.max_db - self.min_db)\n",
    "\n",
    "        normalized_batch = normalized_array.reshape(x.shape)\n",
    "\n",
    "        return normalized_batch\n",
    "\n",
    "    def cache_all(self):\n",
    "        self.cache = True\n",
    "        for idx in range(len(self)):\n",
    "            self.__getitem__(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "\n",
    "        if self.cache_dir is not None:\n",
    "            cache_filename = f\"{os.path.basename(audio_path)}_{self.num_samples}.npy\"\n",
    "            cache_path = os.path.join(self.cache_dir, cache_filename)\n",
    "\n",
    "        if self.cache_dir is not None and os.path.isfile(cache_path):\n",
    "            try:\n",
    "                mel_spec = np.load(cache_path)\n",
    "            except Exception as e:\n",
    "                raise IOError(f\"Failed to read file: {cache_path}. Error: {e}\")\n",
    "        else:\n",
    "            mel_spec = self.get_spec(audio_path)\n",
    "\n",
    "            # Normalize mel_spec\n",
    "            mel_spec = self.normalize(mel_spec)\n",
    "            mel_spec = np.expand_dims(mel_spec, axis=0)\n",
    "\n",
    "            # Save mel spectrogram to cache\n",
    "            if self.cache_dir is not None:\n",
    "                np.save(cache_path, mel_spec)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            mel_spec = self.transform(mel_spec)\n",
    "\n",
    "        folder, filename = os.path.split(audio_path)\n",
    "        basedir, bird = os.path.split(folder)\n",
    "\n",
    "        return mel_spec, bird, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e8fa23-2691-445a-8ca2-8caba122de27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T09:24:19.895571Z",
     "iopub.status.busy": "2024-03-14T09:24:19.895114Z",
     "iopub.status.idle": "2024-03-14T09:24:19.918708Z",
     "shell.execute_reply": "2024-03-14T09:24:19.918158Z",
     "shell.execute_reply.started": "2024-03-14T09:24:19.895541Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 batch_size=64,\n",
    "                 validation_split=0.2,\n",
    "                 num_workers=10,\n",
    "                 bird_name=None,\n",
    "                 transform=None,\n",
    "                 num_samples=65_500,\n",
    "                 min_db=-80,\n",
    "                 max_db=0,\n",
    "                 cache=True,\n",
    "                 seed=0\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.validation_split = validation_split\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "        self.cache = cache\n",
    "        self.seed = seed\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = BirdClefDataset(self.root_dir, bird_name=self.bird_name)\n",
    "        self.normalize = dataset.normalize\n",
    "        self.denormalize = dataset.denormalize\n",
    "        if stage == 'fit' or stage is None:\n",
    "            train_dataset, validation_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                              (1 - self.validation_split, self.validation_split),\n",
    "                                                                             torch.Generator().manual_seed(self.seed))\n",
    "            self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "            self.validation_loader = DataLoader(validation_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.validation_loader\n",
    "\n",
    "    def illustration_dataloader(self, batch_size):\n",
    "        illustrative_dataset = self.validation_loader.dataset\n",
    "        illustrative_loader = DataLoader(illustrative_dataset, batch_size=batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        return illustrative_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659f12ae-13d7-46e5-802e-07e0bbb0d3f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T09:24:19.919687Z",
     "iopub.status.busy": "2024-03-14T09:24:19.919284Z",
     "iopub.status.idle": "2024-03-14T09:24:19.955813Z",
     "shell.execute_reply": "2024-03-14T09:24:19.955222Z",
     "shell.execute_reply.started": "2024-03-14T09:24:19.919665Z"
    }
   },
   "outputs": [],
   "source": [
    "root_directory = 'train_audio'\n",
    "data_module = BirdClefDataModule(root_directory, batch_size=128)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcba302-7e86-43cd-88b8-498171c9faaf",
   "metadata": {},
   "source": [
    "## VAE inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e4f3dc-3199-4436-b448-103d2f69bf18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T09:24:19.957190Z",
     "iopub.status.busy": "2024-03-14T09:24:19.957017Z",
     "iopub.status.idle": "2024-03-14T09:24:19.961324Z",
     "shell.execute_reply": "2024-03-14T09:24:19.960587Z",
     "shell.execute_reply.started": "2024-03-14T09:24:19.957174Z"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def __init__(self, size=16, channels=128):\n",
    "        super(UnFlatten, self).__init__()\n",
    "        self.size = size\n",
    "        self.channels = channels\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), self.channels, self.size, self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba40048-eb2d-4d37-9c93-a2519f8f90fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T09:24:19.962749Z",
     "iopub.status.busy": "2024-03-14T09:24:19.962189Z",
     "iopub.status.idle": "2024-03-14T09:24:19.973369Z",
     "shell.execute_reply": "2024-03-14T09:24:19.972850Z",
     "shell.execute_reply.started": "2024-03-14T09:24:19.962710Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAEGANLoss(torch.nn.modules.loss._Loss):\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, beta=1.0, size_average=None, reduce=None, reduction: str = 'mean', reconstruction_loss=F.mse_loss, reconstruction_decoder_weight=100):\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "        self.beta = beta\n",
    "        self.reconstruction_loss = reconstruction_loss\n",
    "        self.reconstruction_decoder_weight = reconstruction_decoder_weight\n",
    "\n",
    "    def forward(self, reconstructed_x, x, mean, logvar, sampled_classif, reconstructed_classif, original_classif):\n",
    "        reconstruction_loss = self.reconstruction_loss(reconstructed_x, x, reduction=self.reduction)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp(), dim=1)\n",
    "        if self.reduction != 'none':\n",
    "            kl_loss = torch.mean(kl_loss) if self.reduction == 'mean' else torch.sum(kl_loss)\n",
    "\n",
    "        loss_encoder = reconstruction_loss + self.beta * kl_loss\n",
    "\n",
    "        loss_discriminator_samples = F.binary_cross_entropy_with_logits(\n",
    "            sampled_classif,\n",
    "            torch.ones_like(sampled_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "        loss_discriminator_original = F.binary_cross_entropy_with_logits(\n",
    "            original_classif,\n",
    "            torch.zeros_like(original_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "        loss_discriminator_reconstructed = F.binary_cross_entropy_with_logits(\n",
    "            reconstructed_classif,\n",
    "            torch.ones_like(reconstructed_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "        loss_discriminator = 1/4 * (loss_discriminator_samples + loss_discriminator_original * 2 + loss_discriminator_reconstructed)\n",
    "\n",
    "\n",
    "        loss_decoder_reconstructed = F.binary_cross_entropy_with_logits(\n",
    "            reconstructed_classif,\n",
    "            torch.zeros_like(reconstructed_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "\n",
    "        loss_decoder_samples = F.binary_cross_entropy_with_logits(\n",
    "            sampled_classif,\n",
    "            torch.zeros_like(sampled_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "\n",
    "        loss_decoder = (loss_decoder_samples +\n",
    "                        loss_decoder_reconstructed +\n",
    "                        reconstruction_loss * self.reconstruction_decoder_weight)/(self.reconstruction_decoder_weight+2)\n",
    "\n",
    "        return (\n",
    "            loss_encoder,\n",
    "            loss_decoder,\n",
    "            loss_discriminator,\n",
    "            reconstruction_loss,\n",
    "            kl_loss,\n",
    "            loss_decoder_reconstructed,\n",
    "            loss_decoder_samples,\n",
    "            loss_discriminator_samples,\n",
    "            loss_discriminator_original,\n",
    "            loss_discriminator_reconstructed\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2019ebe6-7141-46cf-a349-08b63f6ef869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T09:24:19.974511Z",
     "iopub.status.busy": "2024-03-14T09:24:19.974202Z",
     "iopub.status.idle": "2024-03-14T09:24:20.014401Z",
     "shell.execute_reply": "2024-03-14T09:24:20.013754Z",
     "shell.execute_reply.started": "2024-03-14T09:24:19.974489Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAEGAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=1,\n",
    "        img_size=256,\n",
    "        hidden_size=4096,\n",
    "        layers=[16, 32, 64],\n",
    "        learning_rate=0.001,\n",
    "        lr_decay=1,\n",
    "        beta=1e-3,\n",
    "        activation=nn.ReLU,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        reconstruction_loss=F.mse_loss,\n",
    "        reconstruction_decoder_weight=100,\n",
    "        generate_on_epoch=4,\n",
    "        reconstruct_on_epoch=4,\n",
    "        generator_too_good=.6, # value of the discriminator's loss above which the generator shouldn't be trained\n",
    "        discriminator_too_good=.3, # value of the discriminator's loss above which the discriminator shouldn't be trained\n",
    "        seed=0\n",
    "    ):\n",
    "        super(VAEGAN, self).__init__()\n",
    "\n",
    "        if img_size % (2**len(layers)) != 0:\n",
    "            raise ValueError(\"An image of size {image_size} with {len(layers)} layers won't be reconstructed with the correct size\")\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = self.build_encoder(input_channels, layers, activation)\n",
    "        last_conv_size = (img_size // (2**(len(layers)+1)))**2 * layers[-1]\n",
    "        self.mean_layer = nn.Linear(last_conv_size, self.hidden_size)\n",
    "        self.logvar_layer = nn.Linear(last_conv_size, self.hidden_size)\n",
    "        self.decoder = self.build_decoder(input_channels, layers, img_size, last_conv_size, activation)\n",
    "        self.discriminator = self.build_discriminator(input_channels, layers, activation, last_conv_size)\n",
    "\n",
    "        self.generator_too_good = generator_too_good\n",
    "        self.discriminator_too_good = discriminator_too_good\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.optim = optimizer\n",
    "        self.criterion = VAEGANLoss(\n",
    "            beta=beta,\n",
    "            reconstruction_loss=reconstruction_loss,\n",
    "            reconstruction_decoder_weight=reconstruction_decoder_weight\n",
    "        )\n",
    "        self.generate_on_epoch = generate_on_epoch\n",
    "        self.reconstruct_on_epoch = reconstruct_on_epoch\n",
    "        self.seed = seed\n",
    "\n",
    "        self.prev_disc_loss = None\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def build_discriminator(self, input_channels, channels_list, activation, last_conv_size):\n",
    "        layers = []\n",
    "        in_channels = input_channels\n",
    "        for out_channels in channels_list:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(activation())\n",
    "            in_channels = out_channels\n",
    "        layers.append(Flatten())\n",
    "        layers.append(nn.Linear(last_conv_size, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def build_encoder(self, input_channels, channels_list, activation):\n",
    "        layers = []\n",
    "        in_channels = input_channels\n",
    "        for out_channels in channels_list:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(activation())\n",
    "            #layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "            #layers.append(activation())\n",
    "            #layers.append(nn.MaxPool2d(2))\n",
    "            in_channels = out_channels\n",
    "        layers.append(Flatten())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def build_decoder(self, input_channels, channels_list, img_size, last_conv_size, activation):\n",
    "        layers = [\n",
    "            nn.Linear(self.hidden_size, last_conv_size),\n",
    "            UnFlatten(img_size // (2**(len(channels_list)+1)), channels_list[-1])\n",
    "        ]\n",
    "        for in_channels, out_channels in zip(channels_list[::-1], channels_list[-2::-1]+[input_channels]):\n",
    "            layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "            layers.append(activation())\n",
    "            #layers.append(nn.Upsample(scale_factor=2, mode=\"bilinear\"))\n",
    "            #layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "            #layers.append(activation())\n",
    "        layers.pop()\n",
    "        layers.append(nn.Sigmoid())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def sample(self, mean, logvar, seed=None):\n",
    "        std = logvar.exp()\n",
    "        if seed is None:\n",
    "            epsilon = torch.randn_like(std)\n",
    "        else:\n",
    "            gen = torch.Generator(device=self.device).manual_seed(seed)\n",
    "            epsilon = torch.empty_like(std).normal_(generator=gen)\n",
    "        z = mean + std*epsilon\n",
    "        return z\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, originals):\n",
    "        # reconstruct from inputs\n",
    "        mean, logvar = self.encode(originals)\n",
    "        z = self.sample(mean, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "\n",
    "        # generate from scratch\n",
    "        mean_ = torch.zeros([originals.shape[0], self.hidden_size]).to(self.device)\n",
    "        logvar_ = torch.zeros([originals.shape[0], self.hidden_size]).to(self.device)\n",
    "        z_ = self.sample(mean_, logvar_)\n",
    "        sampled = self.decode(z_)\n",
    "\n",
    "        # discriminate\n",
    "        logits_originals = self.discriminator(originals)\n",
    "        logits_reconstructed = self.discriminator(reconstructed)\n",
    "        logits_sampled = self.discriminator(sampled)\n",
    "\n",
    "        return reconstructed, sampled, mean, logvar, logits_originals, logits_reconstructed, logits_sampled\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if isinstance(self.learning_rate, tuple):\n",
    "            lr_encoder, lr_decoder, lr_discriminator = self.learning_rate\n",
    "        else:\n",
    "            lr_encoder = lr_decoder = lr_discriminator = self.learning_rate\n",
    "\n",
    "        if isinstance(self.lr_decay, tuple):\n",
    "            lr_decay_encoder, lr_decay_decoder, lr_decay_discriminator = self.lr_decay\n",
    "        else:\n",
    "            lr_decay_encoder = lr_decay_decoder = lr_decay_discriminator = self.lr_decay\n",
    "\n",
    "        optimizer_encoder = self.optim(\n",
    "            list(self.encoder.parameters()) + list(self.mean_layer.parameters()) + list(self.logvar_layer.parameters()),\n",
    "            lr=lr_encoder\n",
    "        )\n",
    "        optimizer_decoder = self.optim(self.decoder.parameters(), lr=lr_decoder)\n",
    "        optimizer_discriminator = self.optim(self.discriminator.parameters(), lr=lr_discriminator)\n",
    "\n",
    "        scheduler_encoder = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer_encoder, gamma=lr_decay_encoder),\n",
    "        }\n",
    "\n",
    "        scheduler_decoder = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer_decoder, gamma=lr_decay_decoder),\n",
    "        }\n",
    "\n",
    "        scheduler_discriminator = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer_discriminator, gamma=lr_decay_discriminator),\n",
    "        }\n",
    "\n",
    "        return (\n",
    "            [\n",
    "                optimizer_encoder,\n",
    "                optimizer_decoder,\n",
    "                optimizer_discriminator\n",
    "            ],\n",
    "            [\n",
    "                scheduler_encoder,\n",
    "                scheduler_decoder,\n",
    "                scheduler_discriminator\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        optimizer_encoder, optimizer_decoder, optimizer_discriminator = self.optimizers()\n",
    "        #self.toggle_optimizer(optimizer_encoder)\n",
    "        #self.toggle_optimizer(optimizer_decoder)\n",
    "        #self.toggle_optimizer(optimizer_discriminator)\n",
    "        x, birdname, file = batch\n",
    "        reconstructed, sampled, mean, logvar, logits_originals, logits_reconstructed, logits_sampled = self(x)\n",
    "        (loss_encoder,\n",
    "         loss_decoder,\n",
    "         loss_discriminator,\n",
    "         reconstruction_loss,\n",
    "         kl_loss,\n",
    "         loss_decoder_reconstructed,\n",
    "         loss_decoder_samples,\n",
    "         loss_discriminator_samples,\n",
    "         loss_discriminator_original,\n",
    "         loss_discriminator_reconstructed\n",
    "        ) = self.criterion(\n",
    "            reconstructed,\n",
    "            x,\n",
    "            mean,\n",
    "            logvar,\n",
    "                logits_sampled,\n",
    "            logits_reconstructed,\n",
    "            logits_originals\n",
    "        )\n",
    "\n",
    "\n",
    "        optimizer_encoder.zero_grad()\n",
    "        optimizer_decoder.zero_grad()\n",
    "        optimizer_discriminator.zero_grad()\n",
    "\n",
    "        # we apply the loss to the encoder and then prevent it from being affected by other losses\n",
    "        loss_encoder.backward(retain_graph=True)\n",
    "        for group in optimizer_encoder.param_groups:\n",
    "            for param in group['params']:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # we reset the decoder and discriminator that have been affected by the encoder's loss\n",
    "        optimizer_decoder.zero_grad()\n",
    "        optimizer_discriminator.zero_grad()\n",
    "\n",
    "        # we apply the loss to the decoder and then prevent it from being affected by other losses\n",
    "        loss_decoder.backward(retain_graph=True)\n",
    "        for group in optimizer_decoder.param_groups:\n",
    "            for param in group['params']:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # we reset the discriminator that has been affected by the decoder's loss\n",
    "        optimizer_discriminator.zero_grad()\n",
    "        loss_discriminator.backward()\n",
    "\n",
    "        optimizer_encoder.step()\n",
    "        if self.prev_disc_loss is None or self.prev_disc_loss < self.generator_too_good:\n",
    "            optimizer_decoder.step()\n",
    "            self.log('train/active/generator', 1, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "        else:\n",
    "            self.log('train/active/generator', 0, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "        if self.prev_disc_loss is None or self.prev_disc_loss > self.discriminator_too_good:\n",
    "            optimizer_discriminator.step()\n",
    "            self.log('train/active/discriminator', 1, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "        else:\n",
    "            self.log('train/active/discriminator', 0, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        self.prev_disc_loss = loss_discriminator.detach()\n",
    "\n",
    "        # resettings things to thier normal states\n",
    "        for group in optimizer_encoder.param_groups + optimizer_decoder.param_groups:\n",
    "            for param in group['params']:\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.log('train/loss_encoder', loss_encoder, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_decoder', loss_decoder, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_discriminator', loss_discriminator, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        self.log('train/loss_reconstruction', reconstruction_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_reconstruction', kl_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_decoder_reconstructed', loss_decoder_reconstructed, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_decoder_samples', loss_decoder_samples, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_discriminator_samples', loss_discriminator_samples, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_discriminator_original', loss_discriminator_original, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_discriminator_reconstructed', loss_discriminator_reconstructed, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_reconstruction', reconstruction_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        tensorboard = self.logger.experiment\n",
    "        tensorboard.add_histogram(\"train_mean\", mean, self.trainer.num_training_batches * self.current_epoch + batch_idx)\n",
    "        tensorboard.add_histogram(\"train_logvar\", logvar, self.trainer.num_training_batches * self.current_epoch + batch_idx)\n",
    "\n",
    "        tensorboard.add_histogram(\n",
    "            \"pred_originals\",\n",
    "            F.sigmoid(logits_originals),\n",
    "            self.trainer.num_training_batches * self.current_epoch + batch_idx\n",
    "        )\n",
    "        tensorboard.add_histogram(\n",
    "            \"pred_reconstructed\",\n",
    "            F.sigmoid(logits_reconstructed),\n",
    "            self.trainer.num_training_batches * self.current_epoch + batch_idx\n",
    "        )\n",
    "        tensorboard.add_histogram(\n",
    "            \"pred_sampled\",\n",
    "            F.sigmoid(logits_sampled),\n",
    "            self.trainer.num_training_batches* self.current_epoch + batch_idx\n",
    "        )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, birdname, file = batch\n",
    "        reconstructed, sampled, mean, logvar, logits_originals, logits_reconstructed, logits_sampled = self(x)\n",
    "\n",
    "        (loss_encoder,\n",
    "         loss_decoder,\n",
    "         loss_discriminator,\n",
    "         reconstruction_loss,\n",
    "         kl_loss,\n",
    "         loss_decoder_reconstructed,\n",
    "         loss_decoder_samples,\n",
    "         loss_discriminator_samples,\n",
    "         loss_discriminator_original,\n",
    "         loss_discriminator_reconstructed\n",
    "        ) = self.criterion(\n",
    "            reconstructed,\n",
    "            x,\n",
    "            mean,\n",
    "            logvar,\n",
    "                logits_sampled,\n",
    "            logits_reconstructed,\n",
    "            logits_originals\n",
    "        )\n",
    "\n",
    "        self.log('validation/loss_encoder', loss_encoder, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_decoder', loss_decoder, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_discriminator', loss_discriminator, on_epoch=True, on_step=False, batch_size=x.shape[0])\n",
    "\n",
    "        self.log('validation/loss_reconstruction', reconstruction_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_reconstruction', kl_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_decoder_reconstructed', loss_decoder_reconstructed, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_decoder_samples', loss_decoder_samples, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_discriminator_samples', loss_discriminator_samples, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_discriminator_original', loss_discriminator_original, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_discriminator_reconstructed', loss_discriminator_reconstructed, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_reconstruction', reconstruction_loss, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        tensorboard = self.logger.experiment\n",
    "        tensorboard.add_histogram(\"val_mean\", mean, self.trainer.num_val_batches[0] * self.current_epoch + batch_idx)\n",
    "        tensorboard.add_histogram(\"val_logvar\", logvar, self.trainer.num_val_batches[0] * self.current_epoch + batch_idx)\n",
    "\n",
    "        tensorboard.add_histogram(\n",
    "            \"validation/pred_originals\",\n",
    "            F.sigmoid(logits_originals),\n",
    "            self.trainer.num_val_batches[0] * self.current_epoch + batch_idx\n",
    "        )\n",
    "        tensorboard.add_histogram(\n",
    "            \"validation/pred_reconstructed\",\n",
    "            F.sigmoid(logits_reconstructed),\n",
    "            self.trainer.num_val_batches[0] * self.current_epoch + batch_idx\n",
    "        )\n",
    "        tensorboard.add_histogram(\n",
    "            \"validation/pred_sampled\",\n",
    "            F.sigmoid(logits_sampled),\n",
    "            self.trainer.num_val_batches[0] * self.current_epoch + batch_idx\n",
    "        )\n",
    "\n",
    "    def generate_specs(self, n=None):\n",
    "        if n is None:\n",
    "            n = self.trainer.datamodule.batch_size\n",
    "        mean = torch.zeros([n, self.hidden_size]).to(self.device)\n",
    "        logvar = torch.zeros([n, self.hidden_size]).to(self.device)\n",
    "        z = self.sample(mean, logvar, seed=0)\n",
    "        specs = self.decode(z)\n",
    "\n",
    "        return self.trainer.datamodule.denormalize(specs[:, 0])\n",
    "\n",
    "    def spec_to_img(self, spec):\n",
    "        fig, ax = plt.subplots()\n",
    "        img = librosa.display.specshow(spec, x_axis='time', y_axis='mel', sr=32000, ax=ax)\n",
    "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "        ax.set(title='Mel-frequency spectrogram')\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png')\n",
    "        buffer.seek(0)\n",
    "\n",
    "        plt.close(fig)\n",
    "        image = Image.open(buffer)\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1)  # Convert to tensor and adjust dimensions\n",
    "\n",
    "        return image_tensor\n",
    "\n",
    "    def reconstruction_to_img(self, original_spec, reconstructed_spec):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # Create subplots with 1 row and 2 columns\n",
    "        img1 = librosa.display.specshow(original_spec, x_axis='time', y_axis='mel', sr=32000, ax=axs[0])\n",
    "        fig.colorbar(img1, ax=axs[0], format='%+2.0f dB')\n",
    "        axs[0].set(title='Mel-frequency spectrogram - original')\n",
    "\n",
    "        img2 = librosa.display.specshow(reconstructed_spec, x_axis='time', y_axis='mel', sr=32000, ax=axs[1])\n",
    "        fig.colorbar(img2, ax=axs[1], format='%+2.0f dB')\n",
    "        axs[1].set(title='Mel-frequency spectrogram - reconstructed')\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png')\n",
    "        buffer.seek(0)\n",
    "\n",
    "        plt.close(fig)\n",
    "        image = Image.open(buffer)\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1)  # Convert to tensor and adjust dimensions\n",
    "\n",
    "        return image_tensor\n",
    "\n",
    "    def reconstruct(self, batch):\n",
    "        output, _, _, _, _, _, _ = self(batch)\n",
    "        return output\n",
    "\n",
    "    def on_validation_end(self):\n",
    "        tensorboard = self.logger.experiment\n",
    "\n",
    "        specs = self.generate_specs(self.generate_on_epoch)\n",
    "        for i, spec in enumerate(specs):\n",
    "            tensorboard.add_image(f\"generated_spectrogram_{i}\", self.spec_to_img(spec), self.global_step)\n",
    "            audio = librosa.feature.inverse.mel_to_audio(spec)\n",
    "            tensorboard.add_audio(f\"generated_audio_{i}\", audio, self.global_step, 32000)\n",
    "\n",
    "        reconstruction_dataloader = self.trainer.datamodule.illustration_dataloader(self.reconstruct_on_epoch)\n",
    "        originals, birdnames, files = next(iter(reconstruction_dataloader))\n",
    "        originals = originals.to(self.device)\n",
    "        reconstructed = self.reconstruct(originals)\n",
    "        originals = self.trainer.datamodule.denormalize(originals)[:, 0]\n",
    "        reconstructed = self.trainer.datamodule.denormalize(reconstructed)[:, 0]\n",
    "        for i, (original_spec, reconstructed_spec) in enumerate(zip(originals, reconstructed)):\n",
    "            tensorboard.add_image(\n",
    "                f\"reconstructed_spectrogram_{i}\",\n",
    "                self.reconstruction_to_img(original_spec, reconstructed_spec),\n",
    "                self.global_step\n",
    "            )\n",
    "            original_audio = librosa.feature.inverse.mel_to_audio(original_spec)\n",
    "            reconstructed_audio = librosa.feature.inverse.mel_to_audio(reconstructed_spec)\n",
    "            tensorboard.add_audio(f\"original_audio_{i}\", original_audio, self.global_step, 32000)\n",
    "            tensorboard.add_audio(f\"reconstructed_audio_{i}\", reconstructed_audio, self.global_step, 32000)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        for scheduler in self.lr_schedulers():\n",
    "            scheduler.step()\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.logger.log_hyperparams(self.hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a7b366e-9fe7-4211-bf52-44663814c731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T09:24:20.015255Z",
     "iopub.status.busy": "2024-03-14T09:24:20.015058Z",
     "iopub.status.idle": "2024-03-14T09:24:24.936495Z",
     "shell.execute_reply": "2024-03-14T09:24:24.935638Z",
     "shell.execute_reply.started": "2024-03-14T09:24:20.015234Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b93a5838674b9ead4736b6dc58d171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 0\n",
    "pl.seed_everything(seed, workers=True)\n",
    "\n",
    "model = VAEGAN.load_from_checkpoint(\"vaegan/lightning_logs/version_21/checkpoints/epoch=63-step=16479.ckpt\")\n",
    "mean = torch.zeros([1024, model.hidden_size]).to(model.device)\n",
    "logvar = torch.zeros([1024, model.hidden_size]).to(model.device)\n",
    "z = model.sample(mean, logvar, seed=0)\n",
    "specs = model.decode(z)\n",
    "ds = BirdClefDataset(root_dir='train_audio')\n",
    "sample_rate = 32000\n",
    "for i, mel_spec in enumerate(tqdm(specs)):\n",
    "    filename = f\"{i:}.wav\"\n",
    "    if os.path.isfile(f\"vaegan_inference/{filename}\"):\n",
    "        continue\n",
    "    mel_spec = ds.denormalize(mel_spec)\n",
    "    audio = librosa.feature.inverse.mel_to_audio(mel_spec)\n",
    "    audio = librosa.resample(audio, sample_rate, 16000)\n",
    "    sf.write(f\"vaegan_inference/{filename}\", audio.T, 16000, format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c59c53a7-5481-4e23-8551-c5d41b16c2a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T09:24:24.937701Z",
     "iopub.status.busy": "2024-03-14T09:24:24.937377Z",
     "iopub.status.idle": "2024-03-14T09:24:43.312101Z",
     "shell.execute_reply": "2024-03-14T09:24:43.311369Z",
     "shell.execute_reply.started": "2024-03-14T09:24:24.937668Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                         | 4/1024 [00:00<01:11, 14.18it/s]2024-03-14 10:24:28.788714: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-14 10:24:28.788739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-14 10:24:28.789686: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-14 10:24:28.796366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 10:24:28.808153: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-14 10:24:28.808179: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-14 10:24:28.809122: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-14 10:24:28.815400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 10:24:28.974993: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-14 10:24:28.975022: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-14 10:24:28.976343: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-14 10:24:28.985606: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 10:24:29.092271: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-14 10:24:29.092295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-14 10:24:29.093185: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-14 10:24:29.099308: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "100%|██████████████████████████████████████| 1024/1024 [00:08<00:00, 127.22it/s]\n",
      "2024-03-14 10:24:33.388552: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-14 10:24:33.444350: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-14 10:24:33.444636: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-14 10:24:33.446739: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-14 10:24:33.447070: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-14 10:24:33.447446: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-14 10:24:33.566102: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-14 10:24:33.566460: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-14 10:24:33.566746: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-14 10:24:33.566914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4595 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "/home/mateo/mambaforge/envs/aquatk/lib/python3.11/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1697: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/home/mateo/mambaforge/envs/aquatk/lib/python3.11/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:318: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 10:24:33.975434: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "  0%|                                                  | 0/1024 [00:00<?, ?it/s]2024-03-14 10:24:35.057647: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-03-14 10:24:35.272181: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-14 10:24:38.520748: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 566.38MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-14 10:24:39.407183: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-14 10:24:39.442234: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-14 10:24:39.442301: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 603.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-14 10:24:39.482491: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-14 10:24:39.524514: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-14 10:24:40.074953: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-14 10:24:40.075001: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 603.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-14 10:24:40.104568: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-14 10:24:40.111804: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-14 10:24:40.123165: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "100%|██████████████████████████████████████| 1024/1024 [00:08<00:00, 125.33it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"vaegan_embeddings.npy\"):\n",
    "    vggish_extractor = VGGish(checkpoint_path=\"vggish_model.ckpt\", pca_params_path=\"vggish_pca_params.npz\")\n",
    "    vaegan_embeddings = vggish_extractor.get_embeddings(\"vaegan_inference\")\n",
    "    with open('vaegan_embeddings.npy', 'wb') as f:\n",
    "        np.save(f, vaegan_embeddings)\n",
    "    vggish_extractor.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795db1e-a97a-4c9a-9819-789097969ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aquatk",
   "language": "python",
   "name": "aquatk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
