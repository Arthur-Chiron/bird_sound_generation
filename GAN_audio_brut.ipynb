{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0edadbb-b51b-46d4-9f5f-b09dffb0f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Conv1D, ReLU, Dropout, Flatten, AveragePooling1D, Input\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca961a0-5861-4f19-a84a-5b47fc16bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_DIM = 500\n",
    "SAMPLE_RATE = 32000\n",
    "DURATION = 5\n",
    "AUDIO_DIM = SAMPLE_RATE * DURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672b637-0376-4cc8-aa78-46967c440a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Model\n",
    "def generator(noise_dim, audio_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(noise_dim,)))\n",
    "    model.add(Dense(noise_dim))\n",
    "    model.add(LeakyReLU(negative_slope=0.01))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Reshape((noise_dim, 1)))\n",
    "\n",
    "    model.add(Conv1D(16, 20, padding='same'))\n",
    "    model.add(ReLU())\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Conv1D(32, 25, padding='same'))\n",
    "    model.add(ReLU())\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Conv1D(64, 50, padding='same'))\n",
    "    model.add(ReLU())\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Dropout(rate=0.1))\n",
    " \n",
    "    model.add(Conv1D(320, 100, padding='same'))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(Flatten())\n",
    "    return model\n",
    "\n",
    "# Discriminator Model\n",
    "def discriminator(audio_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(audio_dim,)))\n",
    "    model.add(Reshape((audio_dim, 1)))\n",
    "    model.add(Conv1D(32, 100, strides=7, padding='valid'))\n",
    "    model.add(ReLU())\n",
    "    model.add(AveragePooling1D(4))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Conv1D(16, 50, strides=5, padding='valid'))\n",
    "    model.add(ReLU())\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Conv1D(8, 25, strides=3, padding='valid'))\n",
    "    model.add(ReLU())\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(negative_slope=0.01))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28457bc4-d419-48a0-9395-26865656fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "print(gpu_devices)\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94096a2d-fbd4-44f1-998b-15ada6aed299",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = generator(NOISE_DIM, AUDIO_DIM)\n",
    "G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561fc2c2-674d-43f6-bd10-c104957ae461",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = discriminator(AUDIO_DIM)\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9134809-0ffc-44e8-b2a1-ac81789afb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X = []\n",
    "    NB_BIRDS = 1\n",
    "    i = 0\n",
    "    for root, dirs, _ in os.walk('./birdclef-2023/train_audio'):\n",
    "        for dir in dirs[:NB_BIRDS]:  # A CHANGER\n",
    "            bird_path = os.path.join(root, dir)\n",
    "            for _, _, files in os.walk(bird_path):\n",
    "                for file in files:\n",
    "                    sound_path = os.path.join(bird_path, file)\n",
    "                    audio_data, _ = librosa.load(sound_path)\n",
    "                    duration = librosa.get_duration(y=audio_data, sr=SAMPLE_RATE)\n",
    "                    if duration > DURATION:\n",
    "                        audio_data = audio_data[:AUDIO_DIM]\n",
    "                    else:\n",
    "                        audio_data = np.pad(audio_data, (0, AUDIO_DIM - int(duration * SAMPLE_RATE)), 'constant')\n",
    "                    X.append(audio_data)\n",
    "            print(f'bird dir number {i}')\n",
    "            i += 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b94ec5-36de-43b3-a552-2b2f9b0302b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.compile(loss='binary_crossentropy',\n",
    "          optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef15c92-5e1b-4956-a943-a590bc4fcb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Input(shape=(NOISE_DIM,))\n",
    "audio = G(z)\n",
    "validity = D(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab22e8-5ac2-4b7e-ad99-dd64eb1bb945",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN = Model(z, validity)\n",
    "GAN.compile(loss='binary_crossentropy',\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6f5a3-62d6-477f-88bd-127ba91d5953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour entraîner le GAN\n",
    "def train(iterations, batch_size):\n",
    "    # Charger les données d'oiseaux réels\n",
    "    real_data = load_data()\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        # -----------------------\n",
    "        # Entraînement du discriminateur\n",
    "        # -----------------------\n",
    "\n",
    "        # Générer un batch d'échantillons aléatoires de l'espace latent\n",
    "        noise = np.random.normal(0, 1, (batch_size, NOISE_DIM))\n",
    "        # Générer un batch d'audio à partir du générateur\n",
    "        generated_audio = G.predict(noise)\n",
    "\n",
    "        # Sélectionner un batch d'audio réel au hasard\n",
    "        idx = np.random.randint(0, len(real_data), batch_size)\n",
    "        real_audio = np.array([real_data[i] for i in idx])\n",
    "\n",
    "        # Étiquettes pour l'entraînement du discriminateur\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        # Entraîner le discriminateur sur les vrais et faux échantillons\n",
    "        d_loss_real = D.train_on_batch(real_audio, valid)\n",
    "        d_loss_fake = D.train_on_batch(generated_audio, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # -----------------------\n",
    "        # Entraînement du générateur\n",
    "        # -----------------------\n",
    "\n",
    "        # Générer un batch d'échantillons aléatoires de l'espace latent\n",
    "        noise = np.random.normal(0, 1, (batch_size, NOISE_DIM))\n",
    "        # Étiquettes pour l'entraînement du générateur (vraies images)\n",
    "        valid = np.ones((batch_size, 1))\n",
    "\n",
    "        # Entraîner le générateur (via le GAN) en essayant de tromper le discriminateur\n",
    "        g_loss = GAN.train_on_batch(noise, valid)\n",
    "\n",
    "        # Afficher les progrès\n",
    "        print(f\"Iteration: {iteration}, Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}\")\n",
    "        \n",
    "\n",
    "# Paramètres d'entraînement\n",
    "iterations = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Entraîner le GAN\n",
    "train(iterations, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0131d97a-79c0-4790-b65c-89a221906d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, (batch_size, NOISE_DIM))\n",
    "# Générer un batch d'audio à partir du générateur\n",
    "generated_audio = G.predict(noise)\n",
    "\n",
    "display(Audio(generated_audio[0], rate=SAMPLE_RATE, autoplay=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0cefe2-d532-4754-b84d-5816e21d05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = load_data()\n",
    "display(Audio(real_data[0], rate=SAMPLE_RATE, autoplay=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0facee7-fa3e-4fef-9f4e-8d3da2a55fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
