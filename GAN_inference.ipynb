{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9ff7ca-7410-4e5d-aec3-ee0c470ae250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:19:41.743679Z",
     "iopub.status.busy": "2024-03-14T22:19:41.743325Z",
     "iopub.status.idle": "2024-03-14T22:19:48.067578Z",
     "shell.execute_reply": "2024-03-14T22:19:48.066865Z",
     "shell.execute_reply.started": "2024-03-14T22:19:41.743641Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Audio\n",
    "from PIL import Image\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch import loggers as pl_loggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ac407-8e06-438f-a051-37cfa6ce8c36",
   "metadata": {},
   "source": [
    "## Dataset embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29643a79-d117-48c6-8cde-bc9aef1bda77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:19:48.069436Z",
     "iopub.status.busy": "2024-03-14T22:19:48.069060Z",
     "iopub.status.idle": "2024-03-14T22:19:48.089458Z",
     "shell.execute_reply": "2024-03-14T22:19:48.088872Z",
     "shell.execute_reply.started": "2024-03-14T22:19:48.069412Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        bird_name=None,\n",
    "        transform=None,\n",
    "        num_samples=65_500,\n",
    "        min_db=-80,\n",
    "        max_db=0,\n",
    "        cache=True\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "\n",
    "        if cache:\n",
    "            self.cache_dir = self.root_dir + \"_cache\"\n",
    "            os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        else:\n",
    "            self.cache_dir = None\n",
    "        self.bird_folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "        if bird_name is not None:\n",
    "            self.bird_folders = [bird_name]\n",
    "\n",
    "        self.audio_files = []\n",
    "\n",
    "        for bird_folder in self.bird_folders:\n",
    "            bird_path = os.path.join(root_dir, bird_folder)\n",
    "            audio_files = [os.path.join(bird_path, file) for file in os.listdir(bird_path) if file.endswith('.ogg')]\n",
    "            self.audio_files.extend(audio_files)\n",
    "\n",
    "    def get_spec(self, audio_path):\n",
    "        waveform, sample_rate = librosa.load(audio_path, sr=None, mono=True)\n",
    "\n",
    "        if len(waveform) < self.num_samples:\n",
    "            pad_amount = self.num_samples - len(waveform)\n",
    "            waveform = np.pad(waveform, (0, pad_amount))\n",
    "        else:\n",
    "            waveform = waveform[:self.num_samples]\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=waveform, sr=sample_rate)\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        return mel_spec\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x- self.min_db) / (self.max_db - self.min_db)\n",
    "\n",
    "    def denormalize(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.cpu().detach().numpy()\n",
    "\n",
    "        flattened_array = x.reshape((x.shape[0], -1))\n",
    "\n",
    "        min_batch_values = flattened_array.min(axis=-1, keepdims=True)\n",
    "        max_batch_values = flattened_array.max(axis=-1, keepdims=True)\n",
    "\n",
    "        normalized_array = self.min_db + ((flattened_array - min_batch_values) / (max_batch_values - min_batch_values)) * (self.max_db - self.min_db)\n",
    "\n",
    "        normalized_batch = normalized_array.reshape(x.shape)\n",
    "\n",
    "        return normalized_batch\n",
    "\n",
    "    def cache_all(self):\n",
    "        self.cache = True\n",
    "        for idx in range(len(self)):\n",
    "            self.__getitem__(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "\n",
    "        if self.cache_dir is not None:\n",
    "            cache_filename = f\"{os.path.basename(audio_path)}_{self.num_samples}.npy\"\n",
    "            cache_path = os.path.join(self.cache_dir, cache_filename)\n",
    "\n",
    "        if self.cache_dir is not None and os.path.isfile(cache_path):\n",
    "            try:\n",
    "                mel_spec = np.load(cache_path)\n",
    "            except Exception as e:\n",
    "                raise IOError(f\"Failed to read file: {cache_path}. Error: {e}\")\n",
    "        else:\n",
    "            mel_spec = self.get_spec(audio_path)\n",
    "\n",
    "            # Normalize mel_spec\n",
    "            mel_spec = self.normalize(mel_spec)\n",
    "            mel_spec = np.expand_dims(mel_spec, axis=0)\n",
    "\n",
    "            # Save mel spectrogram to cache\n",
    "            if self.cache_dir is not None:\n",
    "                np.save(cache_path, mel_spec)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            mel_spec = self.transform(mel_spec)\n",
    "\n",
    "        folder, filename = os.path.split(audio_path)\n",
    "        basedir, bird = os.path.split(folder)\n",
    "\n",
    "        return mel_spec, bird, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e8fa23-2691-445a-8ca2-8caba122de27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:19:48.090637Z",
     "iopub.status.busy": "2024-03-14T22:19:48.090320Z",
     "iopub.status.idle": "2024-03-14T22:19:48.101965Z",
     "shell.execute_reply": "2024-03-14T22:19:48.101450Z",
     "shell.execute_reply.started": "2024-03-14T22:19:48.090616Z"
    }
   },
   "outputs": [],
   "source": [
    "class BirdClefDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 batch_size=64,\n",
    "                 validation_split=0.2,\n",
    "                 num_workers=10,\n",
    "                 bird_name=None,\n",
    "                 transform=None,\n",
    "                 num_samples=65_500,\n",
    "                 min_db=-80,\n",
    "                 max_db=0,\n",
    "                 cache=True,\n",
    "                 seed=0\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.validation_split = validation_split\n",
    "        self.bird_name = bird_name\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "        self.min_db = min_db\n",
    "        self.max_db = max_db\n",
    "        self.cache = cache\n",
    "        self.seed = seed\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = BirdClefDataset(self.root_dir, bird_name=self.bird_name)\n",
    "        self.normalize = dataset.normalize\n",
    "        self.denormalize = dataset.denormalize\n",
    "        if stage == 'fit' or stage is None:\n",
    "            train_dataset, validation_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                              (1 - self.validation_split, self.validation_split),\n",
    "                                                                             torch.Generator().manual_seed(self.seed))\n",
    "            self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "            self.validation_loader = DataLoader(validation_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.validation_loader\n",
    "\n",
    "    def illustration_dataloader(self, batch_size):\n",
    "        illustrative_dataset = self.validation_loader.dataset\n",
    "        illustrative_loader = DataLoader(illustrative_dataset, batch_size=batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        return illustrative_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659f12ae-13d7-46e5-802e-07e0bbb0d3f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:19:48.102852Z",
     "iopub.status.busy": "2024-03-14T22:19:48.102578Z",
     "iopub.status.idle": "2024-03-14T22:19:48.169679Z",
     "shell.execute_reply": "2024-03-14T22:19:48.169124Z",
     "shell.execute_reply.started": "2024-03-14T22:19:48.102813Z"
    }
   },
   "outputs": [],
   "source": [
    "root_directory = 'train_audio'\n",
    "data_module = BirdClefDataModule(root_directory, batch_size=128)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcba302-7e86-43cd-88b8-498171c9faaf",
   "metadata": {},
   "source": [
    "## GAN inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e4f3dc-3199-4436-b448-103d2f69bf18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:19:48.170550Z",
     "iopub.status.busy": "2024-03-14T22:19:48.170316Z",
     "iopub.status.idle": "2024-03-14T22:19:48.176270Z",
     "shell.execute_reply": "2024-03-14T22:19:48.175602Z",
     "shell.execute_reply.started": "2024-03-14T22:19:48.170527Z"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def __init__(self, size=16, channels=128):\n",
    "        super(UnFlatten, self).__init__()\n",
    "        self.size = size\n",
    "        self.channels = channels\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), self.channels, self.size, self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba40048-eb2d-4d37-9c93-a2519f8f90fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:19:48.177953Z",
     "iopub.status.busy": "2024-03-14T22:19:48.177578Z",
     "iopub.status.idle": "2024-03-14T22:19:48.185828Z",
     "shell.execute_reply": "2024-03-14T22:19:48.184973Z",
     "shell.execute_reply.started": "2024-03-14T22:19:48.177930Z"
    }
   },
   "outputs": [],
   "source": [
    "class GANLoss(torch.nn.modules.loss._Loss):\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean'):\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "\n",
    "    def forward(self, sampled_classif, original_classif):\n",
    "        loss_discriminator_samples = F.binary_cross_entropy_with_logits(\n",
    "            sampled_classif,\n",
    "            torch.ones_like(sampled_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "        loss_discriminator_original = F.binary_cross_entropy_with_logits(\n",
    "            original_classif,\n",
    "            torch.zeros_like(original_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "        loss_discriminator = 1/2 * (loss_discriminator_samples + loss_discriminator_original)\n",
    "\n",
    "        loss_generator = F.binary_cross_entropy_with_logits(\n",
    "            sampled_classif,\n",
    "            torch.zeros_like(sampled_classif),\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            loss_discriminator,\n",
    "            loss_generator,\n",
    "            loss_discriminator_samples,\n",
    "            loss_discriminator_original\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2019ebe6-7141-46cf-a349-08b63f6ef869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:19:48.189122Z",
     "iopub.status.busy": "2024-03-14T22:19:48.188766Z",
     "iopub.status.idle": "2024-03-14T22:19:48.214222Z",
     "shell.execute_reply": "2024-03-14T22:19:48.213610Z",
     "shell.execute_reply.started": "2024-03-14T22:19:48.189102Z"
    }
   },
   "outputs": [],
   "source": [
    "class GAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_channels=1,\n",
    "        img_size=256,\n",
    "        input_size=4096,\n",
    "        layers=[16, 32, 64],\n",
    "        learning_rate=0.001,\n",
    "        lr_decay=1,\n",
    "        activation=nn.ReLU,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        generate_on_epoch=4,\n",
    "        generator_too_good=.6, # value of the discriminator's loss above which the generator shouldn't be trained\n",
    "        discriminator_too_good=.3, # value of the discriminator's loss above which the discriminator shouldn't be trained\n",
    "        seed=0\n",
    "    ):\n",
    "        super(GAN, self).__init__()\n",
    "\n",
    "        if img_size % (2**len(layers)) != 0:\n",
    "            raise ValueError(\"An image of size {image_size} with {len(layers)} layers won't work\")\n",
    "\n",
    "        self.input_size = input_size\n",
    "        last_conv_size = (img_size // (2**(len(layers)+1)))**2 * layers[-1]\n",
    "        self.generator = self.build_generator(img_channels, layers, img_size, last_conv_size, activation)\n",
    "        self.discriminator = self.build_discriminator(img_channels, layers, activation, last_conv_size)\n",
    "\n",
    "        self.generator_too_good = generator_too_good\n",
    "        self.discriminator_too_good = discriminator_too_good\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.optim = optimizer\n",
    "        self.criterion = GANLoss()\n",
    "        self.generate_on_epoch = generate_on_epoch\n",
    "        self.seed = seed\n",
    "\n",
    "        self.prev_disc_loss = None\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def build_discriminator(self, input_channels, channels_list, activation, last_conv_size):\n",
    "        layers = []\n",
    "        in_channels = input_channels\n",
    "        for out_channels in channels_list:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(activation())\n",
    "            in_channels = out_channels\n",
    "        layers.append(Flatten())\n",
    "        layers.append(nn.Linear(last_conv_size, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def build_generator(self, input_channels, channels_list, img_size, last_conv_size, activation):\n",
    "        layers = [\n",
    "            nn.Linear(self.input_size, last_conv_size),\n",
    "            UnFlatten(img_size // (2**(len(channels_list)+1)), channels_list[-1])\n",
    "        ]\n",
    "        for in_channels, out_channels in zip(channels_list[::-1], channels_list[-2::-1]+[input_channels]):\n",
    "            layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "            layers.append(activation())\n",
    "            #layers.append(nn.Upsample(scale_factor=2, mode=\"bilinear\"))\n",
    "            #layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "            #layers.append(activation())\n",
    "        layers.pop()\n",
    "        layers.append(nn.Sigmoid())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def sample(self, batch_size, seed=None):\n",
    "        if seed is None:\n",
    "            z = torch.randn((batch_size, self.input_size), device=self.device)\n",
    "        else:\n",
    "            gen = torch.Generator(device=self.device).manual_seed(seed)\n",
    "            z = torch.empty((batch_size, self.input_size), device=self.device).normal_(generator=gen)\n",
    "        return z\n",
    "\n",
    "    def generate(self, x):\n",
    "        return self.generator(x)\n",
    "\n",
    "    def forward(self, originals):\n",
    "        # generate\n",
    "        z = self.sample(originals.shape[0])\n",
    "        generated = self.generate(z)\n",
    "\n",
    "        # discriminate\n",
    "        logits_originals = self.discriminator(originals)\n",
    "        logits_generated = self.discriminator(generated)\n",
    "\n",
    "        return generated, logits_originals, logits_generated\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if isinstance(self.learning_rate, tuple):\n",
    "            lr_generator, lr_discriminator = self.learning_rate\n",
    "        else:\n",
    "            lr_generator = lr_discriminator = self.learning_rate\n",
    "\n",
    "        if isinstance(self.lr_decay, tuple):\n",
    "            lr_decay_generator, lr_decay_discriminator = self.lr_decay\n",
    "        else:\n",
    "            lr_decay_generator = lr_decay_discriminator = self.lr_decay\n",
    "\n",
    "        optimizer_generator = self.optim(self.generator.parameters(), lr=lr_generator)\n",
    "        optimizer_discriminator = self.optim(self.discriminator.parameters(), lr=lr_discriminator)\n",
    "\n",
    "        scheduler_generator = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer_generator, gamma=lr_decay_generator),\n",
    "        }\n",
    "        scheduler_discriminator = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ExponentialLR(optimizer_discriminator, gamma=lr_decay_discriminator),\n",
    "        }\n",
    "\n",
    "        return (\n",
    "            [\n",
    "                optimizer_generator,\n",
    "                optimizer_discriminator\n",
    "            ],\n",
    "            [\n",
    "                scheduler_discriminator,\n",
    "                scheduler_discriminator\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        optimizer_generator, optimizer_discriminator = self.optimizers()\n",
    "        #self.toggle_optimizer(optimizer_encoder)\n",
    "        #self.toggle_optimizer(optimizer_decoder)\n",
    "        #self.toggle_optimizer(optimizer_discriminator)\n",
    "        x, birdname, file = batch\n",
    "        generated, logits_originals, logits_generated = self(x)\n",
    "        loss_discriminator, loss_generator, loss_discriminator_samples, loss_discriminator_original = self.criterion(logits_generated, logits_originals)\n",
    "\n",
    "        optimizer_generator.zero_grad()\n",
    "        optimizer_discriminator.zero_grad()\n",
    "\n",
    "        # we apply the loss to the generator and then prevent it from being affected by other losses\n",
    "        loss_generator.backward(retain_graph=True)\n",
    "        for group in optimizer_generator.param_groups:\n",
    "            for param in group['params']:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # we reset the discriminator that has been affected by the generator's loss\n",
    "        optimizer_discriminator.zero_grad()\n",
    "\n",
    "        loss_discriminator.backward()\n",
    "\n",
    "        if self.prev_disc_loss is None or self.prev_disc_loss < self.generator_too_good:\n",
    "            optimizer_generator.step()\n",
    "            self.log('train/active/generator', 1, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "        else:\n",
    "            self.log('train/active/generator', 0, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "        if self.prev_disc_loss is None or self.prev_disc_loss > self.discriminator_too_good:\n",
    "            optimizer_discriminator.step()\n",
    "            self.log('train/active/discriminator', 1, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "        else:\n",
    "            self.log('train/active/discriminator', 0, on_epoch=False, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        self.prev_disc_loss = loss_discriminator.detach()\n",
    "\n",
    "        # resettings things to thier normal states\n",
    "        for group in optimizer_generator.param_groups:\n",
    "            for param in group['params']:\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.log('train/loss_generator', loss_generator, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_discriminator', loss_discriminator, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        self.log('train/loss_discriminator_samples', loss_discriminator_samples, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('train/loss_discriminator_original', loss_discriminator_original, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "        tensorboard = self.logger.experiment\n",
    "\n",
    "        tensorboard.add_histogram(\n",
    "            \"train/pred_originals\",\n",
    "            F.sigmoid(logits_originals),\n",
    "            self.trainer.num_training_batches * self.current_epoch + batch_idx\n",
    "        )\n",
    "\n",
    "        tensorboard.add_histogram(\n",
    "            \"trian/pred_generated\",\n",
    "            F.sigmoid(logits_generated),\n",
    "            self.trainer.num_training_batches* self.current_epoch + batch_idx\n",
    "        )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, birdname, file = batch\n",
    "        generated, logits_originals, logits_generated = self(x)\n",
    "        loss_discriminator, loss_generator, loss_discriminator_samples, loss_discriminator_original = self.criterion(logits_generated, logits_originals)\n",
    "\n",
    "        self.log('validation/loss_generator', loss_generator, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_discriminator', loss_discriminator, on_epoch=True, on_step=False, batch_size=x.shape[0])\n",
    "\n",
    "        self.log('validation/loss_discriminator_samples', loss_discriminator_samples, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "        self.log('validation/loss_discriminator_original', loss_discriminator_original, on_epoch=True, on_step=True, batch_size=x.shape[0])\n",
    "\n",
    "\n",
    "        tensorboard = self.logger.experiment\n",
    "\n",
    "        tensorboard.add_histogram(\n",
    "            \"validation/pred_originals\",\n",
    "            F.sigmoid(logits_originals),\n",
    "            self.trainer.num_val_batches[0] * self.current_epoch + batch_idx\n",
    "        )\n",
    "        tensorboard.add_histogram(\n",
    "            \"validation/pred_generated\",\n",
    "            F.sigmoid(logits_generated),\n",
    "            self.trainer.num_val_batches[0] * self.current_epoch + batch_idx\n",
    "        )\n",
    "\n",
    "    def generate_specs(self, n=None):\n",
    "        if n is None:\n",
    "            n = self.trainer.datamodule.batch_size\n",
    "        z = self.sample(n, seed=0)\n",
    "        specs = self.generate(z)\n",
    "\n",
    "        return self.trainer.datamodule.denormalize(specs[:, 0])\n",
    "\n",
    "    def spec_to_img(self, spec):\n",
    "        fig, ax = plt.subplots()\n",
    "        img = librosa.display.specshow(spec, x_axis='time', y_axis='mel', sr=32000, ax=ax)\n",
    "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "        ax.set(title='Mel-frequency spectrogram')\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png')\n",
    "        buffer.seek(0)\n",
    "\n",
    "        plt.close(fig)\n",
    "        image = Image.open(buffer)\n",
    "        image = image.convert('RGB')\n",
    "        image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1)  # Convert to tensor and adjust dimensions\n",
    "\n",
    "        return image_tensor\n",
    "\n",
    "    def on_validation_end(self):\n",
    "        tensorboard = self.logger.experiment\n",
    "\n",
    "        specs = self.generate_specs(self.generate_on_epoch)\n",
    "        for i, spec in enumerate(specs):\n",
    "            tensorboard.add_image(f\"generated_spectrogram_{i}\", self.spec_to_img(spec), self.global_step)\n",
    "            audio = librosa.feature.inverse.mel_to_audio(spec)\n",
    "            tensorboard.add_audio(f\"generated_audio_{i}\", audio, self.global_step, 32000)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        for scheduler in self.lr_schedulers():\n",
    "            scheduler.step()\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.logger.log_hyperparams(self.hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a7b366e-9fe7-4211-bf52-44663814c731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:19:48.215444Z",
     "iopub.status.busy": "2024-03-14T22:19:48.215087Z",
     "iopub.status.idle": "2024-03-14T22:35:56.238642Z",
     "shell.execute_reply": "2024-03-14T22:35:56.237288Z",
     "shell.execute_reply.started": "2024-03-14T22:19:48.215425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1e617369ce4198ae60ca6842d5ab81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1534523/2752695048.py:15: FutureWarning: Pass orig_sr=32000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  audio = librosa.resample(audio, sample_rate, 16000)\n",
      "/tmp/ipykernel_1534523/2752695048.py:15: FutureWarning: Pass orig_sr=32000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  audio = librosa.resample(audio, sample_rate, 16000)\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "pl.seed_everything(seed, workers=True)\n",
    "\n",
    "model = GAN.load_from_checkpoint(\"gan/lightning_logs/version_8/checkpoints/epoch=99-step=12404.ckpt\")\n",
    "z = model.sample(1024, seed=0)\n",
    "specs = model.generate(z)\n",
    "ds = BirdClefDataset(root_dir='train_audio')\n",
    "sample_rate = 32000\n",
    "for i, mel_spec in enumerate(tqdm(specs)):\n",
    "    filename = f\"{i:}.wav\"\n",
    "    if os.path.isfile(f\"gan_inference/{filename}\"):\n",
    "        continue\n",
    "    mel_spec = ds.denormalize(mel_spec)\n",
    "    audio = librosa.feature.inverse.mel_to_audio(mel_spec)\n",
    "    audio = librosa.resample(audio, sample_rate, 16000)\n",
    "    sf.write(f\"gan_inference/{filename}\", audio.T, 16000, format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795db1e-a97a-4c9a-9819-789097969ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aquatk",
   "language": "python",
   "name": "aquatk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
